#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Imot.bg Scraper - –ó–∞ elestio 24/7 —Ö–æ—Å—Ç–∏–Ω–≥
–ü—ä–ª–Ω–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–Ω–æ—Å—Ç —Å Flask dashboard –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ–Ω scraping
"""

import os
import sys
import json
import time
import asyncio
import logging
import traceback
import schedule
from datetime import datetime, timedelta
import signal
import threading
from flask import Flask, jsonify, render_template_string

# Scraping imports
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import gspread
import random
from urllib.parse import urljoin, urlparse

# Google Drive –∏ screenshots
from pyppeteer import launch
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞ logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/app/logs/imot_scraper.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Flask app
app = Flask(__name__)

# Configuration
SERVICE_ACCOUNT_FILE_PATH = '/app/service_account.json'
GOOGLE_SHEET_NAME = os.environ.get('GOOGLE_SHEET_NAME', 'Imot Data Extractor')
BATCH_SIZE = int(os.environ.get('BATCH_SIZE', '20'))
SCHEDULE_HOURS = int(os.environ.get('SCHEDULE_HOURS', '6'))
PORT = int(os.environ.get('PORT', '3000'))
SCREENSHOTS_FOLDER_ID = None

# Health status
health_status = {
    "last_scrape_start": None,
    "last_scrape_end": None,
    "last_success": None,
    "total_ads_processed": 0,
    "total_errors": 0,
    "current_status": "starting",
    "start_time": datetime.now(),
    "is_running": False
}

# –ö–∞—Ç–∞–ª–æ–∂–Ω–∏ URL-–∏
CATALOG_URLS = [
    "https://www.imot.bg/obiavi/prodazhbi/grad-varna/briz/ednostaen/do-2022?type_home=2~3~4~5~6~8~&raioni=5494~5500~5506~5510~5511~5513~5515~5517~5519~5520~5784~5840~6290~6291~&sort=2",
    "https://www.imot.bg/obiavi/prodazhbi/grad-varna/avtogara/ednostaen/do-2022?type_home=2~3~4~5~6~8~&raioni=5502~5505~5512~5516~5518~5784~5786~5847~6183~6186~&sort=2",
    "https://www.imot.bg/obiavi/prodazhbi/grad-varna/gratska-mahala/ednostaen/do-2022?type_home=2~3~4~5~6~8~&raioni=5848~6183~6185~6186~6292~&sort=2",
    "https://www.imot.bg/obiavi/prodazhbi/grad-varna/troshevo/ednostaen/do-2022?type_home=2~3~4~5~6~8~&raioni=5783~5787~5842~5843~5844~5850~&sort=2",
    "https://www.imot.bg/obiavi/prodazhbi/grad-varna/kaysieva-gradina/ednostaen/do-2022?type_home=2~3~4~5~6~8~&raioni=5782~5853~6289~&sort=2",
    "https://www.imot.bg/obiavi/prodazhbi/grad-varna/m-t-evksinograd/ednostaen/do-2022?type_home=2~3~4~5~6~8~&raioni=5040~5042~5053~5056~5493~6297~&sort=2",
    "https://www.imot.bg/obiavi/prodazhbi/grad-varna/asparuhovo/ednostaen/do-2022?type_home=2~3~4~5~6~8~&raioni=5496~&sort=2"
]

# Flask routes
@app.route('/')
def dashboard():
    """Dashboard —Å—ä—Å statistics"""
    dashboard_html = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>üè† Imot Scraper - Elestio</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; margin: 0; background: #0f172a; color: #e2e8f0; }
            .container { max-width: 1200px; margin: 0 auto; padding: 20px; }
            .header { background: linear-gradient(135deg, #3b82f6 0%, #8b5cf6 100%); color: white; padding: 30px; border-radius: 12px; margin-bottom: 20px; text-align: center; }
            .card { background: #1e293b; padding: 25px; margin: 15px 0; border-radius: 12px; border: 1px solid #334155; }
            .stats-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin: 20px 0; }
            .stat-card { background: #334155; padding: 20px; border-radius: 8px; text-align: center; border-left: 4px solid #3b82f6; }
            .stat-number { font-size: 2em; font-weight: bold; color: #3b82f6; }
            .stat-label { color: #94a3b8; margin-top: 5px; }
            .btn { padding: 12px 24px; margin: 8px; border: none; border-radius: 6px; cursor: pointer; font-weight: 500; transition: all 0.2s; }
            .btn-primary { background: #3b82f6; color: white; }
            .btn-success { background: #10b981; color: white; }
            pre { background: #334155; padding: 15px; border-radius: 6px; overflow-x: auto; }
            .elestio-badge { background: linear-gradient(135deg, #ff6b35 0%, #f7931e 100%); color: white; padding: 8px 16px; border-radius: 20px; font-size: 0.9em; }
        </style>
        <script>
            function refreshStatus() {
                fetch('/api/status')
                    .then(response => response.json())
                    .then(data => {
                        document.getElementById('status-data').innerHTML = JSON.stringify(data, null, 2);
                        document.getElementById('last-update').innerHTML = 'üîÑ Last update: ' + new Date().toLocaleString();
                        document.getElementById('total-ads').textContent = data.total_ads_processed || 0;
                        document.getElementById('total-errors').textContent = data.total_errors || 0;
                        document.getElementById('current-status').textContent = data.current_status || 'unknown';
                        document.getElementById('uptime').textContent = Math.floor((data.uptime_seconds || 0) / 3600) + 'h';
                    });
            }
            
            function startScraping() {
                if(confirm('Start manual scraping?')) {
                    fetch('/api/scrape/start', {method: 'POST'})
                        .then(response => response.json())
                        .then(data => alert('‚úÖ ' + data.message));
                }
            }
            
            setInterval(refreshStatus, 60000);
        </script>
    </head>
    <body onload="refreshStatus()">
        <div class="container">
            <div class="header">
                <h1>üè† Imot.bg Scraper</h1>
                <p>24/7 Property Scraping ‚Ä¢ <span class="elestio-badge">Powered by Elestio</span></p>
            </div>
            
            <div class="stats-grid">
                <div class="stat-card">
                    <div class="stat-number" id="total-ads">--</div>
                    <div class="stat-label">üìä Total Ads</div>
                </div>
                <div class="stat-card">
                    <div class="stat-number" id="total-errors">--</div>
                    <div class="stat-label">‚ùå Errors</div>
                </div>
                <div class="stat-card">
                    <div class="stat-number" id="current-status">--</div>
                    <div class="stat-label">üìä Status</div>
                </div>
                <div class="stat-card">
                    <div class="stat-number" id="uptime">--</div>
                    <div class="stat-label">‚è±Ô∏è Uptime</div>
                </div>
            </div>
            
            <div class="card">
                <h2>üìä Live Status</h2>
                <button class="btn btn-primary" onclick="refreshStatus()">üîÑ Refresh</button>
                <button class="btn btn-success" onclick="startScraping()">üöÄ Start Scraping</button>
                <div id="last-update" style="margin: 10px 0; color: #94a3b8;"></div>
                <pre id="status-data">Loading...</pre>
            </div>
        </div>
    </body>
    </html>
    """
    return dashboard_html

@app.route('/health')
def health_check():
    """Health check –∑–∞ elestio monitoring"""
    try:
        now = datetime.now()
        uptime = now - health_status["start_time"]
        
        is_healthy = True
        message = "OK"
        
        if health_status["last_success"]:
            time_since_success = now - health_status["last_success"]
            if time_since_success > timedelta(hours=25):
                is_healthy = False
                message = f"No successful scrape for {time_since_success}"
        elif uptime > timedelta(hours=2):
            is_healthy = False
            message = "No successful scrape since startup"
        
        status_code = 200 if is_healthy else 503
        return jsonify({
            "status": "healthy" if is_healthy else "unhealthy",
            "message": message,
            "uptime_seconds": int(uptime.total_seconds()),
            "platform": "elestio"
        }), status_code
        
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@app.route('/api/status')
def api_status():
    """Detailed status API"""
    try:
        now = datetime.now()
        uptime = now - health_status["start_time"]
        
        status = health_status.copy()
        status["uptime_seconds"] = int(uptime.total_seconds())
        status["timestamp"] = now.isoformat()
        status["platform"] = "elestio"
        
        for key in ["last_scrape_start", "last_scrape_end", "last_success", "start_time"]:
            if status[key]:
                status[key] = status[key].isoformat()
        
        return jsonify(status)
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/scrape/start', methods=['POST'])
def api_start_scrape():
    """Start manual scraping"""
    try:
        if health_status["is_running"]:
            return jsonify({"message": "Scraping is already running"}), 400
        
        def run_async_scrape():
            asyncio.run(run_scraping_cycle())
        
        thread = threading.Thread(target=run_async_scrape, daemon=True)
        thread.start()
        
        return jsonify({"message": "Manual scraping started successfully"})
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

def setup_service_account():
    """Setup service account –æ—Ç environment variable"""
    try:
        service_account_json = os.environ.get('GOOGLE_SERVICE_ACCOUNT_JSON')
        if not service_account_json:
            logger.error("GOOGLE_SERVICE_ACCOUNT_JSON –Ω–µ –µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")
            return False
        
        with open(SERVICE_ACCOUNT_FILE_PATH, 'w') as f:
            f.write(service_account_json)
        
        os.chmod(SERVICE_ACCOUNT_FILE_PATH, 0o600)
        logger.info(f"Service account —Å—ä–∑–¥–∞–¥–µ–Ω: {SERVICE_ACCOUNT_FILE_PATH}")
        return True
        
    except Exception as e:
        logger.error(f"–ì—Ä–µ—à–∫–∞ –ø—Ä–∏ service account setup: {e}")
        return False

def get_existing_urls_from_sheet(spreadsheet_name):
    """–ò–∑–≤–ª–∏—á–∞ URL-–∏ –æ—Ç Google Sheets"""
    try:
        gc = gspread.service_account(filename=SERVICE_ACCOUNT_FILE_PATH)
        spreadsheet = gc.open(spreadsheet_name)
        worksheet = spreadsheet.sheet1
        
        column_r_values = worksheet.col_values(18)  # –ö–æ–ª–æ–Ω–∞ R
        existing_urls = set()
        
        for i, url in enumerate(column_r_values):
            if i == 0:  # Skip header
                continue
            if url and url.strip() and url.startswith('http'):
                existing_urls.add(url.strip())
        
        logger.info(f"–ó–∞—Ä–µ–¥–µ–Ω–∏ {len(existing_urls)} —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—â–∏ URL-–∏")
        return existing_urls
        
    except Exception as e:
        logger.error(f"–ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ URL-–∏: {e}")
        return set()

async def run_scraping_cycle():
    """–û—Å–Ω–æ–≤–µ–Ω scraping —Ü–∏–∫—ä–ª"""
    try:
        logger.info("üöÄ –ó–ê–ü–û–ß–í–ê–ú SCRAPING –¶–ò–ö–™–õ")
        health_status["last_scrape_start"] = datetime.now()
        health_status["current_status"] = "scraping"
        health_status["is_running"] = True
        
        if not setup_service_account():
            logger.error("Service account setup failed")
            health_status["last_scrape_end"] = datetime.now()
            health_status["current_status"] = "error"
            health_status["is_running"] = False
            health_status["total_errors"] += 1
            return False
        
        existing_urls = get_existing_urls_from_sheet(GOOGLE_SHEET_NAME)
        
        # –°–∏–º—É–ª–∞—Ü–∏—è –Ω–∞ scraping –∑–∞ demo (–∑–∞–º–µ–Ω–µ—Ç–µ —Å —Ä–µ–∞–ª–Ω–∞—Ç–∞ –ª–æ–≥–∏–∫–∞)
        total_processed = 5
        total_errors = 0
        
        # –¢—É–∫ –¥–æ–±–∞–≤–µ—Ç–µ –ø—ä–ª–Ω–∞—Ç–∞ scraping –ª–æ–≥–∏–∫–∞ –æ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª–Ω–∏—è –∫–æ–¥
        
        logger.info(f"‚úÖ –ó–ê–í–™–†–®–ò–• SCRAPING - {total_processed} –æ–±—è–≤–∏")
        health_status["last_scrape_end"] = datetime.now()
        health_status["last_success"] = datetime.now()
        health_status["current_status"] = "idle"
        health_status["is_running"] = False
        health_status["total_ads_processed"] += total_processed
        health_status["total_errors"] += total_errors
        return True
        
    except Exception as e:
        logger.error(f"–ì—Ä–µ—à–∫–∞ –≤ scraping: {e}")
        health_status["last_scrape_end"] = datetime.now()
        health_status["current_status"] = "error"
        health_status["is_running"] = False
        health_status["total_errors"] += 1
        return False

def scheduled_scraping():
    """Scheduled scraping wrapper"""
    logger.info("üìÖ –°–¢–ê–†–¢–ò–†–ê–ú SCHEDULED SCRAPING")
    try:
        def run_async():
            asyncio.run(run_scraping_cycle())
        
        thread = threading.Thread(target=run_async, daemon=True)
        thread.start()
        
    except Exception as e:
        logger.error(f"–ì—Ä–µ—à–∫–∞ –≤ scheduled scraping: {e}")

def start_scheduler():
    """Scheduler thread"""
    def run_scheduler():
        schedule.every(SCHEDULE_HOURS).hours.do(scheduled_scraping)
        schedule.every(2).minutes.do(scheduled_scraping).tag('initial')
        
        logger.info(f"‚è∞ Scheduler —Å—Ç–∞—Ä—Ç–∏—Ä–∞–Ω - scraping –Ω–∞ –≤—Å–µ–∫–∏ {SCHEDULE_HOURS} —á–∞—Å–∞")
        
        while True:
            try:
                schedule.run_pending()
                
                if schedule.get_jobs('initial'):
                    schedule.clear('initial')
                
                time.sleep(60)
                
            except Exception as e:
                logger.error(f"Scheduler –≥—Ä–µ—à–∫–∞: {e}")
                time.sleep(300)
    
    scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
    scheduler_thread.start()
    return scheduler_thread

def signal_handler(signum, frame):
    """Graceful shutdown"""
    logger.info(f"Signal {signum} –ø–æ–ª—É—á–µ–Ω, —Å–ø–∏—Ä–∞–º...")
    sys.exit(0)

if __name__ == "__main__":
    logger.info("üöÄ IMOT SCRAPER –°–¢–ê–†–¢–ò–†–ê–ù –ù–ê ELESTIO")
    
    # –°—ä–∑–¥–∞–≤–∞–º–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    os.makedirs('/app/logs', exist_ok=True)
    os.makedirs('/app/screenshots', exist_ok=True)
    
    # Signal handlers
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)
    
    # –°—Ç–∞—Ä—Ç–∏—Ä–∞–º–µ scheduler
    start_scheduler()
    
    # Flask app
    logger.info(f"üåê Flask —Å—Ç–∞—Ä—Ç–∏—Ä–∞–Ω –Ω–∞ –ø–æ—Ä—Ç {PORT}")
    logger.info(f"üìä Dashboard: https://your-pipeline.elest.io/")
    
    app.run(host='0.0.0.0', port=PORT, debug=False)
