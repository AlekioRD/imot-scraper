–í–∞—à–∏—è—Ç Python —Å–∫—Ä–∏–ø—Ç –µ –¥–æ—Å—Ç–∞ –æ–±—à–∏—Ä–µ–Ω –∏ –¥–æ–±—Ä–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–∞–Ω –∑–∞ —Å–∫—Ä–µ–π–ø–≤–∞–Ω–µ –Ω–∞ Imot.bg, –≤–∫–ª—é—á–∏—Ç–µ–ª–Ω–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–Ω–æ—Å—Ç–∏ –∫–∞—Ç–æ –∏–∑–≤–ª–∏—á–∞–Ω–µ –æ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∫–∞—Ç–∞–ª–æ–∑–∏, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏—è, –ø—Ä–∞–≤–µ–Ω–µ –Ω–∞ —Å–∫—Ä–∏–π–Ω—à–æ—Ç–∏ –∏ –∑–∞–ø–∏—Å –Ω–∞ –¥–∞–Ω–Ω–∏ –≤ Google Sheets, –∫–∞–∫—Ç–æ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞ –¥—É–±–ª–∏—Ä–∞–Ω–∏ URL –∞–¥—Ä–µ—Å–∏.

–û—Å–Ω–æ–≤–Ω–∏—è—Ç –ø—Ä–æ–±–ª–µ–º, –∫–æ–π—Ç–æ —Ç—Ä—è–±–≤–∞ –¥–∞ —Å–µ —Ä–µ—à–∏, –∑–∞ –¥–∞ —Ä–∞–±–æ—Ç–∏ —Ç–æ–∑–∏ —Å–∫—Ä–∏–ø—Ç –≤—ä–≤ –≤–∞—à–∞—Ç–∞ Docker —Å—Ä–µ–¥–∞ –Ω–∞ Elest.io, –µ, —á–µ —Ç–æ–π –µ **–ø—Ä–æ–µ–∫—Ç–∏—Ä–∞–Ω –¥–∞ —Ä–∞–±–æ—Ç–∏ –ø—Ä–µ–¥–∏–º–Ω–æ –≤ Google Colab**. –¢–æ–≤–∞ –µ –≤–∏–¥–Ω–æ –æ—Ç –∏–º–ø–æ—Ä—Ç–∏ –∫–∞—Ç–æ `from google.colab import drive, files` –∏ –∏–∑–ø–æ–ª–∑–≤–∞–Ω–µ—Ç–æ –Ω–∞ `/content/service_account.json` –∫–∞—Ç–æ –ø—ä—Ç –∑–∞ –∞–∫–∞—É–Ω—Ç–∞.

–ï—Ç–æ –∫–∞–∫–≤–æ —Ç—Ä—è–±–≤–∞ –¥–∞ –ø—Ä–æ–º–µ–Ω—è, –∑–∞ –¥–∞ –≥–æ –∞–¥–∞–ø—Ç–∏—Ä–∞–º –∑–∞ Elest.io (–±–∞–∑–∏—Ä–∞–Ω–∞ –Ω–∞ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏) –∏ –¥–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–∞–º `Flask` –∑–∞ `healthcheck`, –∫–∞–∫—Ç–æ –æ–±—Å—ä–¥–∏—Ö–º–µ –ø–æ-—Ä–∞–Ω–æ:

**–ü—Ä–æ–º–µ–Ω–∏, –∫–æ–∏—Ç–æ —â–µ –Ω–∞–ø—Ä–∞–≤—è:**

1.  **–ü—Ä–µ–º–∞—Ö–≤–∞–Ω–µ –Ω–∞ Google Colab —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∏ —á–∞—Å—Ç–∏:**
      * –ü—Ä–µ–º–∞—Ö–≤–∞–Ω–µ –Ω–∞ `google.colab` –∏–º–ø–æ—Ä—Ç–∏ –∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–Ω–æ—Å—Ç–∏ (`drive.mount`, `files.upload`).
      * –ü—Ä–æ–º—è–Ω–∞ –Ω–∞ –ø—ä—Ç—è –∑–∞ `SERVICE_ACCOUNT_FILE_PATH` –¥–∞ –±—ä–¥–µ –ø—Ä–æ–º–µ–Ω–ª–∏–≤–∞ –Ω–∞ —Å—Ä–µ–¥–∞—Ç–∞, –∫–æ—è—Ç–æ —â–µ —Å–µ –ø–æ–¥–∞–≤–∞ –æ—Ç Docker.
2.  **–ò–Ω—Ç–µ–≥—Ä–∏—Ä–∞–Ω–µ –Ω–∞ Flask –∑–∞ `healthcheck`:**
      * –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ Flask –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ.
      * –î–µ—Ñ–∏–Ω–∏—Ä–∞–Ω–µ –Ω–∞ `/health` –µ–Ω–¥–ø–æ–π–Ω—Ç, –∫–æ–π—Ç–æ –≤—Ä—ä—â–∞ "OK".
      * –°—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ Flask —Å—ä—Ä–≤—ä—Ä–∞ –≤ –æ—Ç–¥–µ–ª–Ω–∞ –Ω–∏—à–∫–∞, –∑–∞ –¥–∞ –Ω–µ –±–ª–æ–∫–∏—Ä–∞ –æ—Å–Ω–æ–≤–Ω–∞—Ç–∞ –ª–æ–≥–∏–∫–∞ –Ω–∞ —Å–∫—Ä–µ–π–ø—ä—Ä–∞.
3.  **–ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–∞–Ω–µ —á—Ä–µ–∑ –ø—Ä–æ–º–µ–Ω–ª–∏–≤–∏ –Ω–∞ —Å—Ä–µ–¥–∞—Ç–∞:**
      * –ò–∑–ø–æ–ª–∑–≤–∞–Ω–µ –Ω–∞ `os.getenv()` –∑–∞ `GOOGLE_SHEET_NAME` –∏ `BATCH_SIZE`, `SCHEDULE_HOURS` (–∫–æ–∏—Ç–æ –≤–µ—á–µ —Å–∞ –≤ `docker-compose.yml`), `PORT`.
4.  **–ö–æ—Ä–∏–≥–∏—Ä–∞–Ω–µ –Ω–∞ –ª–æ–≥–∏–∫–∞—Ç–∞ –∑–∞ –∏–Ω—Å—Ç–∞–ª–∏—Ä–∞–Ω–µ –Ω–∞ `pyppeteer` –∏ Chromium:**
      * –õ–æ–≥–∏–∫–∞—Ç–∞ —Å `subprocess.check_call([sys.executable, "-m", "pip", "install", "pyppeteer"])` –∏ `pyppeteer-install` (–≤ `Dockerfile`) –µ –ø—Ä–∞–≤–∏–ª–Ω–∞ –∑–∞ Docker, –Ω–æ —â–µ –ø—Ä–µ–º–∞—Ö–Ω–∞ —Ä—ä—á–Ω–∞—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∏–Ω—Å—Ç–∞–ª–∞—Ü–∏—è –≤ —Å–∞–º–∏—è Python —Å–∫—Ä–∏–ø—Ç, —Ç—ä–π –∫–∞—Ç–æ —Ç–æ–≤–∞ —Å–µ –≥—Ä–∏–∂–∏ `Dockerfile`.
5.  **–õ–æ–∫–∞–ª–Ω–∏ –ø—ä—Ç–∏—â–∞ –∑–∞ —Å–∫—Ä–∏–π–Ω—à–æ—Ç–∏:**
      * –ü—Ä–æ–º—è–Ω–∞ –Ω–∞ `output_folder` –∑–∞ —Å–∫—Ä–∏–π–Ω—à–æ—Ç–∏ –æ—Ç `/content/temp_imot_screenshots/` –∫—ä–º `/app/screenshots/` (–∫–∞–∫—Ç–æ –µ –¥–µ—Ñ–∏–Ω–∏—Ä–∞–Ω–æ –≤—ä–≤ `Dockerfile` –∏ `docker-compose.yml` –∫–∞—Ç–æ volume).
      * –ü—Ä–æ–º—è–Ω–∞ –Ω–∞ `logs` –æ—Ç `/app/logs` (–∫–æ–µ—Ç–æ –≤–µ—á–µ –µ –æ–∫, –Ω–æ —â–µ –≥–æ –Ω–∞–ø—Ä–∞–≤—è —è–≤–Ω–æ).
6.  **–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ —Å–µ—Ä–≤–∏–∑–Ω–∏—è –∞–∫–∞—É–Ω—Ç:** –í–º–µ—Å—Ç–æ –¥–∞ —Ç—ä—Ä—Å–∏ —Ñ–∞–π–ª `/content/service_account.json`, —â–µ –æ—á–∞–∫–≤–∞ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ—Ç–æ –Ω–∞ service account JSON –¥–∞ –±—ä–¥–µ –ø–æ–¥–∞–¥–µ–Ω–æ –∫–∞—Ç–æ –ø—Ä–æ–º–µ–Ω–ª–∏–≤–∞ –Ω–∞ —Å—Ä–µ–¥–∞—Ç–∞ `GOOGLE_SERVICE_ACCOUNT_JSON`.
7.  **–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞ –≥–ª–∞–≤–Ω–∞—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –¥–∞ —Ä–∞–±–æ—Ç–∏ –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ:** –©–µ –∏–∑–ø–æ–ª–∑–≤–∞–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ç–∞ `schedule` (–∫–æ—è—Ç–æ –≤–µ—á–µ –µ –≤ `requirements.txt`) –∑–∞ –¥–∞ –∏–∑–ø—ä–ª–Ω—è–≤–∞–º —Å–∫—Ä–µ–π–ø—ä—Ä–∞ –Ω–∞ –≤—Å–µ–∫–∏ `SCHEDULE_HOURS` —á–∞—Å–∞.

-----

–ï—Ç–æ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–∞–Ω–∏—è—Ç `main.py` —Ñ–∞–π–ª:

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Imot.bg –ö–∞—Ç–∞–ª–æ–≥ Scraper - –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∏–∑–≤–ª–∏—á–∞–Ω–µ –æ—Ç —Ü–µ–ª–∏ –∫–∞—Ç–∞–ª–æ–∑–∏
–ê–¥–∞–ø—Ç–∏—Ä–∞–Ω –∑–∞ Docker/Elest.io —Å—Ä–µ–¥–∞ —Å Flask healthcheck
–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —á—Ä–µ–∑ –ø—Ä–æ–º–µ–Ω–ª–∏–≤–∏ –Ω–∞ —Å—Ä–µ–¥–∞—Ç–∞.
"""

import nest_asyncio
nest_asyncio.apply()

import os
import sys
import subprocess
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import datetime
import gspread
import asyncio
import time
import traceback
import json
import random
from urllib.parse import urljoin, urlparse

# Flask –∑–∞ healthcheck
from flask import Flask
import threading
import schedule # –ó–∞ –ø–ª–∞–Ω–∏—Ä–∞–Ω–µ –Ω–∞ –∑–∞–¥–∞—á–∏

print("üÜï –ó–ê–ü–û–ß–í–ê–ú –ö–ê–¢–ê–õ–û–ñ–ï–ù –ö–û–î - –ê–í–¢–û–ú–ê–¢–ò–ß–ù–û –ò–ó–í–õ–ò–ß–ê–ù–ï –û–¢ –ú–ù–û–ñ–ï–°–¢–í–û –ö–ê–¢–ê–õ–û–ó–ò!")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∏–Ω—Å—Ç–∞–ª–∏—Ä–∞–Ω–µ –Ω–∞ pyppeteer - —Ç–∞–∑–∏ —á–∞—Å—Ç –≤–µ—á–µ –µ –≤ Dockerfile, –Ω–æ —è –æ—Å—Ç–∞–≤—è–º –∑–∞ —Å–∏–≥—É—Ä–Ω–æ—Å—Ç, –∞–∫–æ —Å–µ –∏–∑–ø—ä–ª–Ω—è–≤–∞ –∏–∑–≤—ä–Ω Docker
try:
    import pyppeteer
    print("‚úÖ pyppeteer –µ –Ω–∞–ª–∏—á–µ–Ω.")
except ImportError:
    print("üì¶ –ò–Ω—Å—Ç–∞–ª–∏—Ä–∞–º pyppeteer (—Ç—Ä—è–±–≤–∞ –¥–∞ –µ –∏–Ω—Å—Ç–∞–ª–∏—Ä–∞–Ω –æ—Ç Dockerfile)...")
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "pyppeteer"])
        import pyppeteer
        print("‚úÖ pyppeteer —É—Å–ø–µ—à–Ω–æ –∏–Ω—Å—Ç–∞–ª–∏—Ä–∞–Ω.")
    except Exception as e:
        print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–Ω—Å—Ç–∞–ª–∏—Ä–∞–Ω–µ –Ω–∞ pyppeteer: {e}")
        print("–ú–æ–ª—è, —É–≤–µ—Ä–µ—Ç–µ —Å–µ, —á–µ pip –µ –Ω–∞–ª–∏—á–µ–Ω –∏ —Ä–∞–±–æ—Ç–∏.")

try:
    from pyppeteer import launch
    from google.oauth2.service_account import Credentials
    from googleapiclient.discovery import build
    from googleapiclient.http import MediaFileUpload
    print("‚úÖ –í—Å–∏—á–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞ —Å–∫—Ä–∏–π–Ω—à–æ—Ç–∏ –∏ Google Drive —Å–∞ –Ω–∞–ª–∏—á–Ω–∏.")
except ImportError as e:
    print(f"‚ö†Ô∏è –õ–∏–ø—Å–≤–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –∑–∞ —Å–∫—Ä–∏–π–Ω—à–æ—Ç–∏/Google Drive API: {e}")
    print("üì¶ –ú–æ–ª—è –∏–Ω—Å—Ç–∞–ª–∏—Ä–∞–π—Ç–µ: pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib pyppeteer")

# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –û–¢ –ü–†–û–ú–ï–ù–õ–ò–í–ò –ù–ê –°–†–ï–î–ê–¢–ê
# (–¢–µ–∑–∏ –ø—Ä–æ–º–µ–Ω–ª–∏–≤–∏ —Å–µ –ø–æ–¥–∞–≤–∞—Ç –æ—Ç docker-compose.yml/Elest.io)
GOOGLE_SERVICE_ACCOUNT_JSON = os.getenv('GOOGLE_SERVICE_ACCOUNT_JSON')
GOOGLE_SHEET_NAME = os.getenv('GOOGLE_SHEET_NAME', 'Imot Data Extractor')
BATCH_SIZE = int(os.getenv('BATCH_SIZE', 20))
SCHEDULE_HOURS = int(os.getenv('SCHEDULE_HOURS', 6))
WEB_SERVER_PORT = int(os.getenv('PORT', 3000))

# –õ–æ–∫–∞–ª–Ω–∏ –ø—ä—Ç–∏—â–∞ –∑–∞ –ª–æ–≥–æ–≤–µ –∏ —Å–∫—Ä–∏–π–Ω—à–æ—Ç–∏ –≤ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
LOGS_DIR = '/app/logs'
SCREENSHOTS_DIR = '/app/screenshots'
# –£–≤–µ—Ä–µ—Ç–µ —Å–µ, —á–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏—Ç–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—Ç
os.makedirs(LOGS_DIR, exist_ok=True)
os.makedirs(SCREENSHOTS_DIR, exist_ok=True)

# –ù–û–í–ê –ö–û–ù–°–¢–ê–ù–¢–ê: ID –ù–ê –ü–ê–ü–ö–ê–¢–ê –ó–ê –°–ö–†–ò–ô–ù–®–û–¢–ò (—â–µ —Å–µ –Ω–∞–º–µ—Ä–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ)
SCREENSHOTS_FOLDER_ID = None

# SERVICE_ACCOUNT_FILE_PATH - —Å—ä–∑–¥–∞–≤–∞–º–µ –≤—Ä–µ–º–µ–Ω–µ–Ω —Ñ–∞–π–ª, –∞–∫–æ JSON –µ –ø–æ–¥–∞–¥–µ–Ω –∫–∞—Ç–æ ENV
SERVICE_ACCOUNT_FILE_PATH = os.path.join(LOGS_DIR, 'service_account.json') # –ò–∑–ø–æ–ª–∑–≤–∞–º–µ LOGS_DIR –∑–∞ –≤—Ä–µ–º–µ–Ω–µ–Ω —Ñ–∞–π–ª

# Flask App –∑–∞ Healthcheck
app = Flask(__name__)

@app.route('/health')
def health_check():
    return "OK", 200

# –§—É–Ω–∫—Ü–∏–∏ –∑–∞ Google Drive –∏ Sheets (–±–µ–∑ –ø—Ä–æ–º–µ–Ω–∏ –≤ –ª–æ–≥–∏–∫–∞—Ç–∞, —Å–∞–º–æ –≤ SERVICE_ACCOUNT_FILE_PATH)
def find_screenshots_folder_id():
    """–ü–û–î–û–ë–†–ï–ù–ê –§–£–ù–ö–¶–ò–Ø: –ù–∞–º–∏—Ä–∞ –∏–ª–∏ —Å—ä–∑–¥–∞–≤–∞ –ø–∞–ø–∫–∏—Ç–µ –∑–∞ —Å–∫—Ä–∏–π–Ω—à–æ—Ç–∏ –≤ Google Drive"""
    global SCREENSHOTS_FOLDER_ID

    print("\nüîç –¢–™–†–°–Ø –ü–ê–ü–ö–ò –í GOOGLE DRIVE...")

    try:
        from googleapiclient.discovery import build
        from google.oauth2.service_account import Credentials

        # –ò–∑–ø–æ–ª–∑–≤–∞–º–µ –ø—ä—Ç—è –∫—ä–º –≤—Ä–µ–º–µ–Ω–Ω–∏—è service_account.json
        SERVICE_ACCOUNT_FILE = SERVICE_ACCOUNT_FILE_PATH
        SCOPES = ['https://www.googleapis.com/auth/drive']

        credentials = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
        drive_service = build('drive', 'v3', credentials=credentials)

        # –¢—ä—Ä—Å–∏–º –ø–∞–ø–∫–∞—Ç–∞ "Imot Data Extractor" (–≥—ä–≤–∫–∞–≤–æ —Ç—ä—Ä—Å–µ–Ω–µ)
        main_folder_id = None
        main_folder_name = None

        possible_names = ["Imot Data Extractor", "ImotDataExtractor", "Imot_Data_Extractor", "imot data extractor"]

        for name in possible_names:
            main_folder_query = f"name='{name}' and mimeType='application/vnd.google-apps.folder'"
            main_folder_result = drive_service.files().list(q=main_folder_query, fields="files(id, name)").execute()
            main_folders = main_folder_result.get('files', [])

            if main_folders:
                main_folder_id = main_folders[0]['id']
                main_folder_name = main_folders[0]['name']
                print(f"‚úÖ –ù–∞–º–µ—Ä–∏—Ö –≥–ª–∞–≤–Ω–∞ –ø–∞–ø–∫–∞ '{main_folder_name}' —Å ID: {main_folder_id}")
                break

        if not main_folder_id:
            print("üìÅ –ù–µ –Ω–∞–º–µ—Ä–∏—Ö –ø–∞–ø–∫–∞ 'Imot Data Extractor'. –°—ä–∑–¥–∞–≤–∞–º —è...")

            main_folder_metadata = {
                'name': 'Imot Data Extractor',
                'mimeType': 'application/vnd.google-apps.folder'
            }

            created_main_folder = drive_service.files().create(body=main_folder_metadata, fields='id').execute()
            main_folder_id = created_main_folder.get('id')
            main_folder_name = 'Imot Data Extractor'
            print(f"‚úÖ –°—ä–∑–¥–∞–¥–æ—Ö –≥–ª–∞–≤–Ω–∞ –ø–∞–ø–∫–∞ 'Imot Data Extractor' —Å ID: {main_folder_id}")

        screenshots_folder_query = f"name='ScreenShotsScraper' and mimeType='application/vnd.google-apps.folder' and '{main_folder_id}' in parents"
        screenshots_result = drive_service.files().list(q=screenshots_folder_query, fields="files(id, name)").execute()
        screenshots_folders = screenshots_result.get('files', [])

        if screenshots_folders:
            SCREENSHOTS_FOLDER_ID = screenshots_folders[0]['id']
            print(f"‚úÖ –ù–∞–º–µ—Ä–∏—Ö 'ScreenShotsScraper' —Å ID: {SCREENSHOTS_FOLDER_ID}")
            return SCREENSHOTS_FOLDER_ID
        else:
            print("üìÅ –ù–µ –Ω–∞–º–µ—Ä–∏—Ö 'ScreenShotsScraper'. –°—ä–∑–¥–∞–≤–∞–º —è...")

            screenshots_folder_metadata = {
                'name': 'ScreenShotsScraper',
                'mimeType': 'application/vnd.google-apps.folder',
                'parents': [main_folder_id]
            }

            created_screenshots_folder = drive_service.files().create(body=screenshots_folder_metadata, fields='id').execute()
            SCREENSHOTS_FOLDER_ID = created_screenshots_folder.get('id')
            print(f"‚úÖ –°—ä–∑–¥–∞–¥–æ—Ö 'ScreenShotsScraper' —Å ID: {SCREENSHOTS_FOLDER_ID}")

            print(f"\nüìÅ –§–ò–ù–ê–õ–ù–ê –°–¢–†–£–ö–¢–£–†–ê:")
            print(f" ¬† My Drive")
            print(f" ¬† ‚îî‚îÄ‚îÄ {main_folder_name} (ID: {main_folder_id})")
            print(f" ¬† ¬† ¬† ‚îî‚îÄ‚îÄ ScreenShotsScraper (ID: {SCREENSHOTS_FOLDER_ID})")

            return SCREENSHOTS_FOLDER_ID

    except Exception as e:
        print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ —Ç—ä—Ä—Å–µ–Ω–µ/—Å—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –ø–∞–ø–∫–∏: {e}")
        traceback.print_exc()
        return None

def get_catalog_name_from_url(url):
    # –û—Ä–∏–≥–∏–Ω–∞–ª–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è
    try:
        if "/briz/" in url:
            return "–ë—Ä–∏–∑"
        elif "/avtogara/" in url:
            return "–ê–≤—Ç–æ–≥–∞—Ä–∞"
        elif "/gratska-mahala/" in url:
            return "–ì—Ä–∞–¥—Å–∫–∞ –º–∞—Ö–∞–ª–∞"
        elif "/troshevo/" in url:
            return "–¢—Ä–æ—à–µ–≤–æ"
        elif "/kaysieva-gradina/" in url:
            return "–ö–∞–π—Å–∏–µ–≤–∞ –≥—Ä–∞–¥–∏–Ω–∞"
        elif "/m-t-evksinograd/" in url:
            return "–º-—Ç –ï–≤–∫—Å–∏–Ω–æ–≥—Ä–∞–¥"
        elif "/asparuhovo/" in url:
            return "–ê—Å–ø–∞—Ä—É—Ö–æ–≤–æ"
        else:
            parts = url.split("/")
            for i, part in enumerate(parts):
                if part == "grad-varna" and i + 1 < len(parts):
                    return parts[i + 1].replace("-", " ").title()
            return "–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω —Ä–∞–π–æ–Ω"
    except:
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω —Ä–∞–π–æ–Ω"

def get_existing_urls_from_sheet(spreadsheet_name):
    print(f"\nüîç –ò–ó–í–õ–ò–ß–ê–ú –°–™–©–ï–°–¢–í–£–í–ê–©–ò URL-–ò –û–¢ –¢–ê–ë–õ–ò–¶–ê–¢–ê '{spreadsheet_name}'")

    try:
        # –ò–∑–ø–æ–ª–∑–≤–∞–º–µ –ø—ä—Ç—è –∫—ä–º –≤—Ä–µ–º–µ–Ω–Ω–∏—è service_account.json
        gc = gspread.service_account(filename=SERVICE_ACCOUNT_FILE_PATH)
        spreadsheet = gc.open(spreadsheet_name)
        worksheet = spreadsheet.sheet1

        column_r_values = worksheet.col_values(18)
        existing_urls = set()
        for i, url in enumerate(column_r_values):
            if i == 0:
                continue
            if url and url.strip() and url.startswith('http'):
                existing_urls.add(url.strip())

        print(f"‚úÖ –ù–∞–º–µ—Ä–∏—Ö {len(existing_urls)} —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—â–∏ URL-–∏ –≤ —Ç–∞–±–ª–∏—Ü–∞—Ç–∞")

        if existing_urls:
            print("üìã –ü—Ä–∏–º–µ—Ä–Ω–∏ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—â–∏ URL-–∏:")
            for i, url in enumerate(list(existing_urls)[:3]):
                print(f" ¬† {i+1}. {url}")
            if len(existing_urls) > 3:
                print(f" ¬† ... –∏ –æ—â–µ {len(existing_urls) - 3}")

        return existing_urls

    except Exception as e:
        print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—â–∏ URL-–∏: {e}")
        traceback.print_exc()
        return set()

def is_url_already_processed(url, existing_urls):
    return url.strip() in existing_urls

def upload_to_drive_screenshots_folder(local_path):
    print(f"‚¨ÜÔ∏è –ö–∞—á–≤–∞–º —Å–∫—Ä–∏–π–Ω—à–æ—Ç '{os.path.basename(local_path)}' –≤ Google Drive...")
    try:
        from googleapiclient.discovery import build
        from google.oauth2.service_account import Credentials
        from googleapiclient.http import MediaFileUpload

        folder_id = SCREENSHOTS_FOLDER_ID
        if not folder_id:
            print("‚ùå –ù—è–º–∞ ID –Ω–∞ –ø–∞–ø–∫–∞ ScreenShotsScraper")
            return None

        # –ò–∑–ø–æ–ª–∑–≤–∞–º–µ –ø—ä—Ç—è –∫—ä–º –≤—Ä–µ–º–µ–Ω–Ω–∏—è service_account.json
        SERVICE_ACCOUNT_FILE = SERVICE_ACCOUNT_FILE_PATH
        SCOPES = ['https://www.googleapis.com/auth/drive']

        credentials = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
        drive_service = build('drive', 'v3', credentials=credentials)

        file_metadata = {
            'name': os.path.basename(local_path),
            'parents': [folder_id]
        }

        media = MediaFileUpload(local_path, mimetype='image/jpeg')

        file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()
        file_id = file.get('id')

        drive_service.permissions().create(
            fileId=file_id,
            body={'role': 'reader', 'type': 'anyone'}
        ).execute()

        drive_link = f"https://drive.google.com/file/d/{file_id}/view"
        print(f"‚úÖ –°–∫—Ä–∏–π–Ω—à–æ—Ç –∫–∞—á–µ–Ω –≤ ScreenShotsScraper –ø–∞–ø–∫–∞: {drive_link}")
        return drive_link

    except Exception as e:
        print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∫–∞—á–≤–∞–Ω–µ –≤ ScreenShotsScraper –ø–∞–ø–∫–∞: {e}")
        traceback.print_exc()
        return None

async def screenshot_final(url, output_folder=SCREENSHOTS_DIR):
    print(f"üì∏ –ü—Ä–∞–≤—è —Å–∫—Ä–∏–π–Ω—à–æ—Ç –Ω–∞: {url}")
    browser = None

    try:
        browser = await launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-gpu',
                '--disable-features=VizDisplayCompositor'
            ]
        )
        page = await browser.newPage()
        await page.setViewport({'width': 1920, 'height': 1080})

        await page.goto(url, {'waitUntil': ['networkidle0'], 'timeout': 60000})
        await asyncio.sleep(3)

        safe_filename = re.sub(r'[^a-zA-Z0-9_\-.]', '_', url.split('/')[-1])[:40]
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        screenshot_name = f"imot_{safe_filename}_{timestamp}.jpeg"
        screenshot_path = os.path.join(output_folder, screenshot_name)

        os.makedirs(output_folder, exist_ok=True)

        await page.screenshot({
            'path': screenshot_path,
            'fullPage': True,
            'type': 'jpeg',
            'quality': 60
        })
        print(f"‚úÖ –°–∫—Ä–∏–π–Ω—à–æ—Ç –∑–∞–ø–∞–∑–µ–Ω –ª–æ–∫–∞–ª–Ω–æ (JPEG, –∫–∞—á–µ—Å—Ç–≤–æ 60): {screenshot_path}")

        drive_link = upload_to_drive_screenshots_folder(screenshot_path)

        try:
            os.remove(screenshot_path)
            print(f"üóëÔ∏è –õ–æ–∫–∞–ª–Ω–∏—è—Ç —Ñ–∞–π–ª –µ –∏–∑—Ç—Ä–∏—Ç")
        except Exception as e:
            print(f"‚ö†Ô∏è –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑—Ç—Ä–∏–≤–∞–Ω–µ –Ω–∞ –ª–æ–∫–∞–ª–µ–Ω —Å–∫—Ä–∏–π–Ω—à–æ—Ç: {e}")

        return drive_link if drive_link else f"–õ–æ–∫–∞–ª–Ω–æ: {screenshot_path}"

    except Exception as e:
        print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ —Å–∫—Ä–∏–π–Ω—à–æ—Ç: {e}")
        traceback.print_exc()
        return None
    finally:
        if browser:
            await browser.close()

def fetch_html_final(url):
    # –û—Ä–∏–≥–∏–Ω–∞–ª–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'bg-BG,bg;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }

    print(f"üåê –ò–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ HTML –æ—Ç: {url}")
    try:
        response = requests.get(url, headers=headers, allow_redirects=True, timeout=25)
        response.raise_for_status()

        for encoding in ['utf-8', 'windows-1251', 'iso-8859-1']:
            try:
                test_content = response.content.decode(encoding)
                if '–∏–º–æ—Ç' in test_content.lower() or '–ø—Ä–æ–¥–∞–≤–∞' in test_content.lower() or '—Ü–µ–Ω–∞' in test_content.lower():
                    response.encoding = encoding
                    print(f"‚úÖ HTML —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω (encoding: {encoding})")
                    return response.text
            except UnicodeDecodeError:
                continue

        response.encoding = 'utf-8'
        return response.text

    except Exception as e:
        print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ HTML –∑–∞ {url}: {e}")
        return None

def clean_text_final(text):
    # –û—Ä–∏–≥–∏–Ω–∞–ª–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è
    if not isinstance(text, str):
        return "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ"
    try:
        if '' in text:
            text = text.replace('', '')
        cleaned_text = re.sub(r'[^\w\s\.,\-_\/\(\):;?!%&#+=\[\]{}|~^<>`@*–∞-—è–ê-–Ø—ë–Å]', '', text)
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
        return cleaned_text if cleaned_text else "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ"
    except Exception:
        return "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ"

def extract_full_description(soup):
    # –û—Ä–∏–≥–∏–Ω–∞–ª–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è
    print(" ¬† üîç –¢—ä—Ä—Å—è –ø—ä–ª–Ω–æ—Ç–æ –æ–ø–∏—Å–∞–Ω–∏–µ...")

    description_div = soup.find('div', id='description_div')
    if description_div:
        description_html = str(description_div)
        description_html = re.sub(r'<br\s*/?>', '\n', description_html)
        temp_soup = BeautifulSoup(description_html, 'html.parser')
        description_text = temp_soup.get_text()
        description_text = re.sub(r'\n+', ' ', description_text)
        description_text = re.sub(r'\s+', ' ', description_text).strip()
        if description_text and len(description_text) > 50:
            print(f" ¬† ‚úÖ –û–¢ description_div: '{description_text[:100]}...' (–æ–±—â–æ {len(description_text)} —Å–∏–º–≤–æ–ª–∞)")
            return clean_text_final(description_text)

    desc_candidates = soup.find_all('div', id=True)
    for div in desc_candidates:
        div_id = div.get('id', '')
        if 'desc' in div_id.lower():
            desc_text = div.get_text(strip=True)
            if len(desc_text) > 100:
                desc_text = re.sub(r'\n+', ' ', desc_text)
                desc_text = re.sub(r'\s+', ' ', desc_text).strip()
                print(f" ¬† ‚úÖ –û–¢ {div_id}: '{desc_text[:100]}...'")
                return clean_text_final(desc_text)

    table_cells = soup.find_all('td')
    for cell in table_cells:
        cell_text = cell.get_text(strip=True)
        if (len(cell_text) > 200 and
            any(word in cell_text.lower() for word in ['–µ—Ç–∞–∂', '—Å–≥—Ä–∞–¥–∞', '–∞–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç', '–∫–æ—Ä–∏–¥–æ—Ä', '—Å–ø–∞–ª–Ω–∏'])):
            desc_text = re.sub(r'\n+', ' ', cell_text)
            desc_text = re.sub(r'\s+', ' ', desc_text).strip()
            print(f" ¬† ‚úÖ –û–¢ —Ç–∞–±–ª–∏—Ü–∞: '{desc_text[:100]}...'")
            return clean_text_final(desc_text)

    print(" ¬† ‚ùå –û–ø–∏—Å–∞–Ω–∏–µ –ù–ï –µ –Ω–∞–º–µ—Ä–µ–Ω–æ")
    return "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ"

def analyze_property_complete(url, html_content):
    # –û—Ä–∏–≥–∏–Ω–∞–ª–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è
    default_date = datetime.datetime.now().strftime("%d/%m/%Y")

    data = {
        "–¢–∏–ø –∏–º–æ—Ç": "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ",
        "–ö–≤–∞—Ä—Ç–∞–ª": "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ",
        "–¶–µ–Ω–∞ (EUR)": "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ",
        "–¶–µ–Ω–∞ –Ω–∞ –º¬≤ (EUR)": "",
        "–ü–ª–æ—â": "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ",
        "–ï—Ç–∞–∂": "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ",
        "–î–∞—Ç–∞ –Ω–∞ –≤—ä–≤–µ–∂–¥–∞–Ω–µ": default_date,
        "–¢–∏–ø —Å—Ç—Ä–æ–∏—Ç–µ–ª—Å—Ç–≤–æ": "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ",
        "–ì–æ–¥–∏–Ω–∞ –Ω–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—Å—Ç–≤–æ": "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ",
        "–û—Å–æ–±–µ–Ω–æ—Å—Ç–∏": "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ",
        "–ü—Ä–æ–¥–∞–≤–∞—á": "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ",
        "–¢–µ–ª–µ—Ñ–æ–Ω –Ω–∞ –ø—Ä–æ–¥–∞–≤–∞—á–∞": "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ",
        "–û–ø–∏—Å–∞–Ω–∏–µ": "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ",
        "–°—ä—Å—Ç–æ—è–Ω–∏–µ": "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ",
        "–°–∫—Ä–∏–π–Ω—à–æ—Ç": "",
        "–¶–µ–Ω–∞ –Ω–∞ —Å–¥–µ–ª–∫–∞—Ç–∞": "",
        "–ö–æ–º–µ–Ω—Ç–∞—Ä–∏": "",
        "URL –∑–∞ –∞–Ω–∞–ª–∏–∑": ""
    }

    if not html_content:
        print(f"‚ùå HTML –µ –ø—Ä–∞–∑–Ω–æ –∑–∞ {url}")
        data["URL –∑–∞ –∞–Ω–∞–ª–∏–∑"] = url
        return {"success": False, "error": "HTML –µ –ø—Ä–∞–∑–Ω–æ.", "data": data}

    soup = BeautifulSoup(html_content, 'html.parser')

    try:
        print(f"\nüîç –ó–ê–ü–û–ß–í–ê–ú –ü–û–î–û–ë–†–ï–ù –ê–ù–ê–õ–ò–ó –ù–ê: {url}")
        print("="*60)

        print("1Ô∏è‚É£ –¢–ò–ü –ò–ú–û–¢:")
        adv_header = soup.find('div', class_='advHeader')
        if adv_header:
            title_div = adv_header.find('div', class_='title')
            if title_div:
                title_text = ''
                for content in title_div.contents:
                    if hasattr(content, 'name') and content.name == 'div':
                        break
                    elif hasattr(content, 'strip'):
                        title_text += content.strip() + ' '

                title_text = title_text.strip()
                if '–ü—Ä–æ–¥–∞–≤–∞' in title_text:
                    property_type = re.sub(r'^.*?–ü—Ä–æ–¥–∞–≤–∞\s+', '', title_text).strip()
                    data["–¢–∏–ø –∏–º–æ—Ç"] = clean_text_final(property_type)
                    print(f" ¬† ‚úÖ '{property_type}'")

        print("2Ô∏è‚É£ –ö–í–ê–†–¢–ê–õ:")
        location_div = soup.find('div', class_='location')
        if location_div:
            location_text = location_div.get_text(strip=True)
            location_text = re.sub(r'\s+', ' ', location_text).strip()
            location_text = re.sub(r'^–≥—Ä–∞–¥\s+–í–∞—Ä–Ω–∞,\s*', '', location_text, flags=re.IGNORECASE)
            data["–ö–≤–∞—Ä—Ç–∞–ª"] = clean_text_final(location_text)
            print(f" ¬† ‚úÖ (—Å–ª–µ–¥ –ø—Ä–µ–º–∞—Ö–≤–∞–Ω–µ –Ω–∞ '–≥—Ä–∞–¥ –í–∞—Ä–Ω–∞, ') '{location_text}'")
        else:
            breadcrumbs = soup.find('div', class_='breadcrumbs')
            if breadcrumbs:
                links = breadcrumbs.find_all('a')
                if len(links) >= 2:
                    location_parts = []
                    for link in links[-2:]:
                        text = clean_text_final(link.get_text().strip())
                        if text and text != "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ":
                            location_parts.append(text)

                    if location_parts:
                        location_combined = ", ".join(location_parts)
                        location_combined = re.sub(r'^–≥—Ä–∞–¥\s+–í–∞—Ä–Ω–∞,\s*', '', location_combined, flags=re.IGNORECASE)
                        data["–ö–≤–∞—Ä—Ç–∞–ª"] = location_combined
                        print(f" ¬† ‚úÖ (–æ—Ç breadcrumbs) '{data['–ö–≤–∞—Ä—Ç–∞–ª']}'")

        print("3Ô∏è‚É£ –¶–ï–ù–ê:")
        title_tag = soup.find('title')
        if title_tag:
            title_text = title_tag.get_text()
            eur_match = re.search(r'([\d\s]+)\s*EUR', title_text)
            if eur_match:
                price_num = re.sub(r'\s+', '', eur_match.group(1))
                if price_num.isdigit():
                    data["–¶–µ–Ω–∞ (EUR)"] = price_num
                    print(f" ¬† ‚úÖ {price_num} EUR")

        print("4Ô∏è‚É£ –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò –û–¢ adParams:")
        ad_params = soup.find('div', class_='adParams')
        if ad_params:
            param_divs = ad_params.find_all('div', recursive=False)
            for i, param_div in enumerate(param_divs):
                div_text = param_div.get_text(strip=True)

                if '–ü–ª–æ—â' in div_text:
                    area_html = str(param_div)
                    area_html = re.sub(r'<sup>2</sup>', '¬≤', area_html)
                    temp_soup = BeautifulSoup(area_html, 'html.parser')
                    full_area_text = temp_soup.get_text(strip=True).replace('\n', ' ')
                    full_area_text = re.sub(r'\s+', ' ', full_area_text).strip()

                    area_match = re.search(r'(\d+)', full_area_text)
                    if area_match:
                        data["–ü–ª–æ—â"] = area_match.group(1)
                        print(f" ¬† ‚úÖ –ü–õ–û–©: '{area_match.group(1)} m¬≤'")

                elif '–ï—Ç–∞–∂' in div_text:
                    full_floor_text = div_text.replace('\n', ' ')
                    full_floor_text = re.sub(r'\s+', ' ', full_floor_text).strip()
                    floor_cleaned = re.sub(r'^–ï—Ç–∞–∂:\s*', '', full_floor_text, flags=re.IGNORECASE)
                    data["–ï—Ç–∞–∂"] = clean_text_final(floor_cleaned)
                    print(f" ¬† ‚úÖ –ï–¢–ê–ñ (–±–µ–∑ '–ï—Ç–∞–∂:'): '{floor_cleaned}'")

                elif '–°—Ç—Ä–æ–∏—Ç–µ–ª—Å—Ç–≤–æ' in div_text:
                    strong_tags = param_div.find_all('strong')

                    if len(strong_tags) >= 1:
                        first_strong = strong_tags[0].get_text(strip=True).rstrip(', ')
                        data["–¢–∏–ø —Å—Ç—Ä–æ–∏—Ç–µ–ª—Å—Ç–≤–æ"] = clean_text_final(first_strong)
                        print(f" ¬† ‚úÖ –°–¢–†–û–ò–¢–ï–õ–°–¢–í–û: '{first_strong}'")

                    if len(strong_tags) >= 2:
                        second_strong = strong_tags[1].get_text(strip=True)
                        data["–ì–æ–¥–∏–Ω–∞ –Ω–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—Å—Ç–≤–æ"] = clean_text_final(second_strong)
                        print(f" ¬† ‚úÖ –ì–û–î–ò–ù–ê: '{second_strong}'")

        print("5Ô∏è‚É£ –¶–ï–ù–ê –ù–ê –ú¬≤ (—â–µ –±—ä–¥–µ —Ñ–æ—Ä–º—É–ª–∞ –≤ Google Sheets):")
        if (data["–¶–µ–Ω–∞ (EUR)"] != "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ" and str(data["–¶–µ–Ω–∞ (EUR)"]).isdigit() and
            data["–ü–ª–æ—â"] != "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ" and str(data["–ü–ª–æ—â"]).isdigit()):
            data["–¶–µ–Ω–∞ –Ω–∞ –º¬≤ (EUR)"] = "=ROUND(C{row}/D{row},0)"
            print(f" ¬† ‚úÖ –©–µ —Å–µ –∏–∑—á–∏—Å–ª–∏ —Å —Ñ–æ—Ä–º—É–ª–∞: =ROUND(C/D,0)")
        else:
            data["–¶–µ–Ω–∞ –Ω–∞ –º¬≤ (EUR)"] = ""

        print("6Ô∏è‚É£ –¢–ï–õ–ï–§–û–ù:")
        phone_div = soup.find('div', class_='phone')
        if phone_div:
            phone_text = phone_div.get_text(strip=True)
            phone_text = re.sub(r'–ò–∑–ø—Ä–∞—Ç–∏\s+E-mail\s+–Ω–∞\s+–ø—Ä–æ–¥–∞–≤–∞—á–∞', '', phone_text).strip()
            phone_numbers = re.findall(r'(\d{10})', phone_text)
            if phone_numbers:
                data["–¢–µ–ª–µ—Ñ–æ–Ω –Ω–∞ –ø—Ä–æ–¥–∞–≤–∞—á–∞"] = phone_numbers[0]
                print(f" ¬† ‚úÖ –û–¢ div.phone: '{phone_numbers[0]}'")
            else:
                data["–¢–µ–ª–µ—Ñ–æ–Ω –Ω–∞ –ø—Ä–æ–¥–∞–≤–∞—á–∞"] = clean_text_final(phone_text)
                print(f" ¬† ‚úÖ –û–¢ div.phone (—Ç–µ–∫—Å—Ç): '{phone_text}'")
        else:
            phone_candidates = soup.find_all('div', class_=True)
            found_phone = False
            for div in phone_candidates:
                div_classes = ' '.join(div.get('class', []))
                if 'phone' in div_classes.lower():
                    phone_text = div.get_text(strip=True)
                    phone_text = re.sub(r'–ò–∑–ø—Ä–∞—Ç–∏\s+E-mail\s+–Ω–∞\s+–ø—Ä–æ–¥–∞–≤–∞—á–∞', '', phone_text).strip()
                    if phone_text:
                        phone_numbers = re.findall(r'(\d{10})', phone_text)
                        if phone_numbers:
                            data["–¢–µ–ª–µ—Ñ–æ–Ω –Ω–∞ –ø—Ä–æ–¥–∞–≤–∞—á–∞"] = phone_numbers[0]
                            print(f" ¬† ‚úÖ –û–¢ {div_classes}: '{phone_numbers[0]}'")
                        else:
                            data["–¢–µ–ª–µ—Ñ–æ–Ω –Ω–∞ –ø—Ä–æ–¥–∞–≤–∞—á–∞"] = clean_text_final(phone_text)
                            print(f" ¬† ‚úÖ –û–¢ {div_classes}: '{phone_text}'")
                        found_phone = True
                        break

            if not found_phone:
                phone_patterns = [r'(0\d{9})']
                all_text = soup.get_text()
                for pattern in phone_patterns:
                    phone_match = re.search(pattern, all_text)
                    if phone_match:
                        found_phone_num = phone_match.group(1)
                        data["–¢–µ–ª–µ—Ñ–æ–Ω –Ω–∞ –ø—Ä–æ–¥–∞–≤–∞—á–∞"] = found_phone_num
                        print(f" ¬† ‚úÖ –û–¢ regex: '{found_phone_num}'")
                        break

        print("7Ô∏è‚É£ –û–ü–ò–°–ê–ù–ò–ï:")
        description = extract_full_description(soup)
        data["–û–ø–∏—Å–∞–Ω–∏–µ"] = description

        print("8Ô∏è‚É£ –ü–†–û–î–ê–í–ê–ß:")
        agency_divs = soup.find_all('div', class_='agency')
        if agency_divs:
            for agency_div in agency_divs:
                agency_text = agency_div.get_text().strip()
                if agency_text:
                    data["–ü—Ä–æ–¥–∞–≤–∞—á"] = clean_text_final(agency_text[:100])
                    print(f" ¬† ‚úÖ –ü—Ä–æ–¥–∞–≤–∞—á: {data['–ü—Ä–æ–¥–∞–≤–∞—á']}")
                    break

        print("9Ô∏è‚É£ –ü–û–î–ì–û–¢–û–í–ö–ê –ó–ê –õ–ò–ù–ö–û–í–ï:")
        data["–°–∫—Ä–∏–π–Ω—à–æ—Ç"] = ""
        data["URL –∑–∞ –∞–Ω–∞–ª–∏–∑"] = url
        print(f" ¬† ‚è≥ –°–∫—Ä–∏–π–Ω—à–æ—Ç —â–µ —Å–µ –Ω–∞–ø—Ä–∞–≤–∏ —Å–ª–µ–¥ –∞–Ω–∞–ª–∏–∑–∞")
        print(f" ¬† ‚úÖ URL –≤ –∫–æ–ª–æ–Ω–∞ R: {url}")

        print(f"\nüìä –§–ò–ù–ê–õ–ï–ù –†–ï–ó–£–õ–¢–ê–¢ –û–¢ –ü–û–î–û–ë–†–ï–ù–ò–Ø –ê–ù–ê–õ–ò–ó:")
        for key, value in data.items():
            status = "‚úÖ" if value not in ["–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ", "–ù–µ –µ —É–∫–∞–∑–∞–Ω–æ", ""] else "‚ùå"
            display_value = str(value)[:50] + "..." if len(str(value)) > 50 else str(value)
            print(f" ¬† {status} {key}: '{display_value}'")

        return {"success": True, "data": data}

    except Exception as e_analyze:
        print(f"‚ùå –ì—Ä–µ—à–∫–∞ –≤ –∞–Ω–∞–ª–∏–∑–∞ –∑–∞ {url}: {e_analyze}")
        print(traceback.format_exc())
        data["URL –∑–∞ –∞–Ω–∞–ª–∏–∑"] = url
        return {"success": False, "error": f"–ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑: {e_analyze}", "data": data}

def extract_ad_links_from_catalog(catalog_url):
    # –û—Ä–∏–≥–∏–Ω–∞–ª–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è
    print(f"\nüîç –ò–ó–í–õ–ò–ß–ê–ú –õ–ò–ù–ö–û–í–ï –û–¢ –ö–ê–¢–ê–õ–û–ì–ê: {catalog_url}")

    html_content = fetch_html_final(catalog_url)
    if not html_content:
        print("‚ùå –ù–µ —É—Å–ø—è—Ö –¥–∞ –∑–∞—Ä–µ–¥—è –∫–∞—Ç–∞–ª–æ–≥–æ–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
        return []

    soup = BeautifulSoup(html_content, 'html.parser')

    ads_container = soup.find('div', class_='ads2023')
    if not ads_container:
        print("‚ùå –ù–µ –Ω–∞–º–µ—Ä–∏—Ö ads2023 –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä")
        return []

    ad_links = []
    ad_items = ads_container.find_all('div', class_='item')
    print(f"üìã –ù–∞–º–µ—Ä–∏—Ö {len(ad_items)} –æ–±—è–≤–∏ –≤ –∫–∞—Ç–∞–ª–æ–≥–∞")

    for i, item in enumerate(ad_items):
        try:
            zaglavie_div = item.find('div', class_='zaglavie')
            if zaglavie_div:
                title_link = zaglavie_div.find('a', class_='title saveSlink')
                if title_link and title_link.get('href'):
                    href = title_link.get('href')

                    if href.startswith('//'):
                        full_url = 'https:' + href
                    elif href.startswith('/'):
                        full_url = 'https://www.imot.bg' + href
                    else:
                        full_url = href

                    ad_links.append(full_url)
                    title_text = title_link.get_text(strip=True)
                    print(f" ¬† {i+1}. {title_text[:60]}...")
                else:
                    print(f" ¬† {i+1}. ‚ùå –ù—è–º–∞ –ª–∏–Ω–∫ –≤ zaglavie")
            else:
                print(f" ¬† {i+1}. ‚ùå –ù—è–º–∞ zaglavie div")

        except Exception as e:
            print(f" ¬† {i+1}. ‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞: {e}")

    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏ {len(ad_links)} –≤–∞–ª–∏–¥–Ω–∏ –ª–∏–Ω–∫–∞ –æ—Ç –∫–∞—Ç–∞–ª–æ–≥–∞")
    return ad_links

def find_next_page_url(catalog_url):
    # –û—Ä–∏–≥–∏–Ω–∞–ª–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è
    print(f"\nüîç –¢–™–†–°–Ø –°–õ–ï–î–í–ê–©–ê –°–¢–†–ê–ù–ò–¶–ê –û–¢: {catalog_url}")

    html_content = fetch_html_final(catalog_url)
    if not html_content:
        print("‚ùå –ù–µ —É—Å–ø—è—Ö –¥–∞ –∑–∞—Ä–µ–¥—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ç–∞ –∑–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏—è")
        return None

    soup = BeautifulSoup(html_content, 'html.parser')

    pagination_wrapper = soup.find('div', class_='pagination-wrapper')
    if not pagination_wrapper:
        print("‚ùå –ù–µ –Ω–∞–º–µ—Ä–∏—Ö pagination-wrapper")
        return None

    pagination = pagination_wrapper.find('div', class_='pagination')
    if not pagination:
        print("‚ùå –ù–µ –Ω–∞–º–µ—Ä–∏—Ö pagination div")
        return None

    next_link = pagination.find('a', class_='next')
    if not next_link:
        all_links = pagination.find_all('a')
        for link in all_links:
            if '–Ω–∞–ø—Ä–µ–¥' in link.get_text().lower():
                next_link = link
                break

    if next_link and next_link.get('href'):
        next_href = next_link.get('href')

        if next_href.startswith('//'):
            next_url = 'https:' + next_href
        elif next_href.startswith('/'):
            next_url = 'https://www.imot.bg' + next_href
        else:
            next_url = next_href

        print(f"‚úÖ –ù–∞–º–µ—Ä–∏—Ö —Å–ª–µ–¥–≤–∞—â–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {next_url}")
        return next_url
    else:
        print("‚úÖ –ù—è–º–∞ —Å–ª–µ–¥–≤–∞—â–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ - –¥–æ—Å—Ç–∏–≥–Ω–∞—Ö –ø–æ—Å–ª–µ–¥–Ω–∞—Ç–∞")
        return None

def check_sheet_access(spreadsheet_name):
    # –û—Ä–∏–≥–∏–Ω–∞–ª–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è
    print(f"\nüìã –ü–†–û–í–ï–†–ö–ê –ù–ê –î–û–°–¢–™–ü–ê –î–û –¢–ê–ë–õ–ò–¶–ê–¢–ê '{spreadsheet_name}'")

    try:
        # –ò–∑–ø–æ–ª–∑–≤–∞–º–µ –ø—ä—Ç—è –∫—ä–º –≤—Ä–µ–º–µ–Ω–Ω–∏—è service_account.json
        gc = gspread.service_account(filename=SERVICE_ACCOUNT_FILE_PATH)
        spreadsheet = gc.open(spreadsheet_name)
        worksheet = spreadsheet.sheet1

        print(f"‚úÖ –¢–∞–±–ª–∏—Ü–∞—Ç–∞ '{spreadsheet_name}' –µ –¥–æ—Å—Ç—ä–ø–Ω–∞")
        print(f"üìù –ê–∫—Ç–∏–≤–µ–Ω –ª–∏—Å—Ç: '{worksheet.title}'")

        all_values = worksheet.get_all_values()
        print(f"üìä –¢–µ–∫—É—â–∏ —Ä–µ–¥–æ–≤–µ –≤ —Ç–∞–±–ª–∏—Ü–∞—Ç–∞: {len(all_values)}")

        return True

    except Exception as e:
        print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –¥–æ—Å—Ç—ä–ø –¥–æ —Ç–∞–±–ª–∏—Ü–∞—Ç–∞: {e}")
        traceback.print_exc()
        return False

def save_data_to_sheets(spreadsheet_name, data_list):
    # –û—Ä–∏–≥–∏–Ω–∞–ª–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è
    print(f"\nüìù –î–û–ë–ê–í–Ø–ù–ï –ù–ê –î–ê–ù–ù–ò –í GOOGLE SHEET: '{spreadsheet_name}'")

    if not data_list:
        print("‚ö†Ô∏è –ù–Ø–ú–ê –î–ê–ù–ù–ò –∑–∞ –∑–∞–ø–∏—Å.")
        return True

    try:
        # –ò–∑–ø–æ–ª–∑–≤–∞–º–µ –ø—ä—Ç—è –∫—ä–º –≤—Ä–µ–º–µ–Ω–Ω–∏—è service_account.json
        with open(SERVICE_ACCOUNT_FILE_PATH, 'r') as f:
            sa_info = json.load(f)
        client_email = sa_info.get('client_email')
        print(f"‚ÑπÔ∏è Service account email: {client_email}")

        gc = gspread.service_account(filename=SERVICE_ACCOUNT_FILE_PATH)
        spreadsheet = gc.open(spreadsheet_name)
        worksheet = spreadsheet.sheet1
        print(f"‚úÖ –û—Ç–≤–æ—Ä–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ '{spreadsheet_name}'")

        all_values = worksheet.get_all_values()
        current_row_count = len(all_values)
        print(f"üìä –¢–µ–∫—É—â–∏ —Ä–µ–¥–æ–≤–µ –≤ —Ç–∞–±–ª–∏—Ü–∞—Ç–∞: {current_row_count}")

        exact_column_order = [
            "–¢–∏–ø –∏–º–æ—Ç",              # A
            "–ö–≤–∞—Ä—Ç–∞–ª",               # B
            "–¶–µ–Ω–∞ (EUR)",            # C
            "–ü–ª–æ—â",                  # D
            "–¶–µ–Ω–∞ –Ω–∞ –º¬≤ (EUR)",      # E
            "–ï—Ç–∞–∂",                  # F
            "–î–∞—Ç–∞ –Ω–∞ –≤—ä–≤–µ–∂–¥–∞–Ω–µ",     # G
            "–¢–∏–ø —Å—Ç—Ä–æ–∏—Ç–µ–ª—Å—Ç–≤–æ",      # H
            "–ì–æ–¥–∏–Ω–∞ –Ω–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—Å—Ç–≤–æ",# I
            "–û—Å–æ–±–µ–Ω–æ—Å—Ç–∏",            # J
            "–ü—Ä–æ–¥–∞–≤–∞—á",              # K
            "–¢–µ–ª–µ—Ñ–æ–Ω –Ω–∞ –ø—Ä–æ–¥–∞–≤–∞—á–∞",  # L
            "–û–ø–∏—Å–∞–Ω–∏–µ",              # M
            "–°—ä—Å—Ç–æ—è–Ω–∏–µ",             # N
            "–°–∫—Ä–∏–π–Ω—à–æ—Ç",             # O
            "–¶–µ–Ω–∞ –Ω–∞ —Å–¥–µ–ª–∫–∞—Ç–∞",      # P
            "–ö–æ–º–µ–Ω—Ç–∞—Ä–∏",             # Q
            "URL –∑–∞ –∞–Ω–∞–ª–∏–∑"          # R
        ]

        print(f"\nüîç –ü–†–û–í–ï–†–ö–ê –ù–ê –°–¢–†–£–ö–¢–£–†–ê–¢–ê:")
        print(f" ¬† –û—á–∞–∫–≤–∞–Ω–∏ –∫–æ–ª–æ–Ω–∏: {len(exact_column_order)} (A-R)")
        for i, col in enumerate(exact_column_order):
            letter = chr(65 + i)
            print(f" ¬† {letter}: {col}")

        rows_to_write = []
        for i, data_dict in enumerate(data_list):
            row = []
            next_row_number = current_row_count + i + 1

            print(f"\nüìù –ü–æ–¥–≥–æ—Ç–≤—è–º —Ä–µ–¥ {i+1} (—â–µ –±—ä–¥–µ —Ä–µ–¥ {next_row_number} –≤ —Ç–∞–±–ª–∏—Ü–∞—Ç–∞):")

            for j, column_name in enumerate(exact_column_order):
                column_letter = chr(65 + j)

                if column_name == "–¶–µ–Ω–∞ –Ω–∞ –º¬≤ (EUR)":
                    price = data_dict.get("–¶–µ–Ω–∞ (EUR)", "")
                    area = data_dict.get("–ü–ª–æ—â", "")
                    if (price not in ["–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ", ""] and
                        area not in ["–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ", ""] and
                        str(price).isdigit() and str(area).isdigit()):
                        value = f"=ROUND(C{next_row_number}/D{next_row_number},0)"
                    else:
                        value = ""
                elif column_name == "–°–∫—Ä–∏–π–Ω—à–æ—Ç":
                    value = str(data_dict.get(column_name, ""))
                    if not value or value == "":
                        value = "‚ùå –ù—è–º–∞ —Å–∫—Ä–∏–π–Ω—à–æ—Ç"
                else:
                    value = str(data_dict.get(column_name, ""))
                    if value == "–ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–æ":
                        value = ""

                row.append(value)

                if j < 8:
                    print(f" ¬† {column_letter} ({column_name}): '{value}'")

            if len(row) != len(exact_column_order):
                print(f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –†–µ–¥ {i+1} –∏–º–∞ {len(row)} –∫–æ–ª–æ–Ω–∏, –æ—á–∞–∫–≤–∞–Ω–∏ {len(exact_column_order)}")

            rows_to_write.append(row)

        print(f"\n‚ÑπÔ∏è –î–æ–±–∞–≤—è–º {len(rows_to_write)} —Ä–µ–¥(–∞) —Å –¢–û–ß–ù–û {len(exact_column_order)} –∫–æ–ª–æ–Ω–∏ –≤—Å–µ–∫–∏")
        print(f"üìç –î–∞–Ω–Ω–∏—Ç–µ —â–µ –∑–∞–ø–æ—á–Ω–∞—Ç –æ—Ç –∫–æ–ª–æ–Ω–∞ A –∏ —â–µ –∑–∞–≤—ä—Ä—à–∞—Ç –≤ –∫–æ–ª–æ–Ω–∞ R")

        for i, row in enumerate(rows_to_write):
            try:
                worksheet.append_row(row, value_input_option='USER_ENTERED')
                print(f"‚úÖ –ó–∞–ø–∏—Å–∞–Ω —Ä–µ–¥ {i+1}")
                time.sleep(0.5)
            except Exception as e_row:
                print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å –Ω–∞ —Ä–µ–¥ {i+1}: {e_row}")

        time.sleep(2)
        all_values_after = worksheet.get_all_values()
        print(f"‚úÖ‚úÖ‚úÖ –£–°–ü–ï–®–ù–û –î–û–ë–ê–í–ï–ù–ò {len(rows_to_write)} –†–ï–î–ê!")
        print(f"üìä –û–±—â–æ —Ä–µ–¥–æ–≤–µ –≤ —Ç–∞–±–ª–∏—Ü–∞—Ç–∞ —Å–µ–≥–∞: {len(all_values_after)}")
        print(f"üìç –î–∞–Ω–Ω–∏—Ç–µ —Å–∞ –≤ –∫–æ–ª–æ–Ω–∏ A-R (–Ω–µ V!)")

        return True

    except Exception as e_save:
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ù–û: –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å: {e_save}")
        traceback.print_exc()
        return False

# –ù–û–í–ê –ö–û–ù–°–¢–ê–ù–¢–ê: –°–ü–ò–°–™–ö –° –í–°–ò–ß–ö–ò –ö–ê–¢–ê–õ–û–ñ–ù–ò URL-–ò
# (–¢–æ–∑–∏ —Å–ø–∏—Å—ä–∫ –º–æ–∂–µ –¥–∞ —Å–µ –ø–æ–¥–∞–≤–∞ –∏ –∫–∞—Ç–æ –ø—Ä–æ–º–µ–Ω–ª–∏–≤–∞ –Ω–∞ —Å—Ä–µ–¥–∞—Ç–∞, –∞–∫–æ –µ —Ç–≤—ä—Ä–¥–µ –¥—ä–ª—ä–≥, –Ω–æ –∑–∞—Å–µ–≥–∞ –≥–æ –æ—Å—Ç–∞–≤—è–º —Ç—É–∫)
CATALOG_URLS = [
    "https://www.imot.bg/obiavi/prodazhbi/grad-varna/briz/ednostaen/do-2022?type_home=2~3~4~5~6~8~&raioni=5494~5500~5506~5510~5511~5513~5515~5517~5519~5520~5784~5840~6290~6291~&sort=2",
    "https://www.imot.bg/obiavi/prodazhbi/grad-varna/avtogara/ednostaen/do-2022?type_home=2~3~4~5~6~8~&raioni=5502~5505~5512~5516~5518~5784~5786~5847~6183~6186~&sort=2",
    "https://www.imot.bg/obiavi/prodazhbi/grad-varna/gratska-mahala/ednostaen/do-2022?type_home=2~3~4~5~6~8~&raioni=5848~6183~6185~6186~6292~&sort=2",
    "https://www.imot.bg/obiavi/prodazhbi/grad-varna/troshevo/ednostaen/do-2022?type_home=2~3~4~5~6~8~&raioni=5783~5787~5842~5843~5844~5850~&sort=2",
    "https://www.imot.bg/obiavi/prodazhbi/grad-varna/kaysieva-gradina/ednostaen/do-2022?type_home=2~3~4~5~6~8~&raioni=5782~5853~6289~&sort=2",
    "https://www.imot.bg/obiavi/prodazhbi/grad-varna/m-t-evksinograd/ednostaen/do-2022?type_home=2~3~4~5~6~8~&raioni=5040~5042~5053~5056~5493~6297~&sort=2",
    "https://www.imot.bg/obiavi/prodazhbi/grad-varna/asparuhovo/ednostaen/do-2022?type_home=2~3~4~5~6~8~&raioni=5496~&sort=2"
]

async def process_single_catalog(catalog_url, catalog_index, total_catalogs, existing_urls, google_sheet_name):
    # –û—Ä–∏–≥–∏–Ω–∞–ª–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è
    catalog_name = get_catalog_name_from_url(catalog_url)

    print(f"\n" + "="*80)
    print(f"üè™ –ó–ê–ü–û–ß–í–ê–ú –ö–ê–¢–ê–õ–û–ì {catalog_index}/{total_catalogs}: {catalog_name}")
    print(f"üîó {catalog_url}")
    print("="*80)

    current_catalog_url = catalog_url
    page_number = 1
    catalog_processed_ads = 0
    catalog_skipped_ads = 0

    accumulated_data = []
    batch_counter = 0

    while current_catalog_url:
        print(f"\n" + "-"*60)
        print(f"üìÑ –ö–ê–¢–ê–õ–û–ì {catalog_name} - –°–¢–†–ê–ù–ò–¶–ê {page_number}")
        print(f"üîó {current_catalog_url}")
        print("-"*60)

        ad_links = extract_ad_links_from_catalog(current_catalog_url)

        if not ad_links:
            print(f"‚ö†Ô∏è –ù—è–º–∞ –æ–±—è–≤–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_number} –æ—Ç –∫–∞—Ç–∞–ª–æ–≥ '{catalog_name}' - –ø—Ä–µ–º–∏–Ω–∞–≤–∞–º –∫—ä–º —Å–ª–µ–¥–≤–∞—â–∞—Ç–∞")
            current_catalog_url = find_next_page_url(current_catalog_url)
            page_number += 1
            continue

        print(f"\nüîÑ –ü–†–û–í–ï–†–ö–ê –ó–ê –î–£–ë–õ–ò–†–ê–ù–ò URL-–ò –ù–ê –°–¢–†–ê–ù–ò–¶–ê {page_number} –û–¢ '{catalog_name}':")
        new_ad_links = []
        skipped_count = 0

        for i, ad_url in enumerate(ad_links):
            if is_url_already_processed(ad_url, existing_urls):
                print(f" ¬† ‚è≠Ô∏è {i+1}. –ü–†–û–ü–£–°–ö–ê–ú (–≤–µ—á–µ –µ –æ–±—Ä–∞–±–æ—Ç–µ–Ω–∞): {ad_url[:60]}...")
                skipped_count += 1
                catalog_skipped_ads += 1
            else:
                print(f" ¬† ‚úÖ {i+1}. –ù–û–í–ê –û–ë–Ø–í–ê: {ad_url[:60]}...")
                new_ad_links.append(ad_url)

        print(f"\nüìä –†–ï–ó–£–õ–¢–ê–¢ –û–¢ –§–ò–õ–¢–†–ò–†–ê–ù–ï–¢–û –ù–ê –°–¢–†–ê–ù–ò–¶–ê {page_number} –û–¢ '{catalog_name}':")
        print(f" ¬† üìã –û–±—â–æ –æ–±—è–≤–∏ –≤ –∫–∞—Ç–∞–ª–æ–≥–∞: {len(ad_links)}")
        print(f" ¬† ‚è≠Ô∏è –ü—Ä–µ—Å–∫–æ—á–µ–Ω–∏ (–≤–µ—á–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—Ç): {skipped_count}")
        print(f" ¬† ‚úÖ –ù–æ–≤–∏ –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞: {len(new_ad_links)}")

        if not new_ad_links:
            print(f"‚ö†Ô∏è –ù—è–º–∞ –Ω–æ–≤–∏ –æ–±—è–≤–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_number} –æ—Ç '{catalog_name}' - –ø—Ä–µ–º–∏–Ω–∞–≤–∞–º –∫—ä–º —Å–ª–µ–¥–≤–∞—â–∞—Ç–∞")
            current_catalog_url = find_next_page_url(current_catalog_url)
            page_number += 1
            continue

        print(f"\nüîÑ –û–ë–†–ê–ë–û–¢–í–ê–ú {len(new_ad_links)} –ù–û–í–ò –û–ë–Ø–í–ò –û–¢ –°–¢–†–ê–ù–ò–¶–ê {page_number} –ù–ê '{catalog_name}'")

        for index, ad_url in enumerate(new_ad_links):
            print(f"\n" + "."*40)
            print(f"üîÑ –ö–ê–¢–ê–õ–û–ì '{catalog_name}' - –û–ë–Ø–í–ê {index + 1}/{len(new_ad_links)} –û–¢ –°–¢–†–ê–ù–ò–¶–ê {page_number}")
            print(f"üìä BATCH: {len(accumulated_data)}/{BATCH_SIZE}")
            print(f"üîó {ad_url}")
            print("."*40)

            html_content = fetch_html_final(ad_url)
            if not html_content:
                print(f"‚ùå –ù–µ –º–æ–∂–∞—Ö –¥–∞ –∏–∑–≤–ª–µ–∫–∞ HTML –æ—Ç –æ–±—è–≤–∞—Ç–∞")
                error_data = {
                    "–¢–∏–ø –∏–º–æ—Ç": "HTML –≥—Ä–µ—à–∫–∞", "–ö–≤–∞—Ä—Ç–∞–ª": "HTML –≥—Ä–µ—à–∫–∞",
                    "–¶–µ–Ω–∞ (EUR)": "HTML –≥—Ä–µ—à–∫–∞", "–ü–ª–æ—â": "HTML –≥—Ä–µ—à–∫–∞",
                    "–¶–µ–Ω–∞ –Ω–∞ –º¬≤ (EUR)": "", "–ï—Ç–∞–∂": "HTML –≥—Ä–µ—à–∫–∞",
                    "–î–∞—Ç–∞ –Ω–∞ –≤—ä–≤–µ–∂–¥–∞–Ω–µ": datetime.datetime.now().strftime("%d/%m/%Y"),
                    "–¢–∏–ø —Å—Ç—Ä–æ–∏—Ç–µ–ª—Å—Ç–≤–æ": "HTML –≥—Ä–µ—à–∫–∞", "–ì–æ–¥–∏–Ω–∞ –Ω–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—Å—Ç–≤–æ": "HTML –≥—Ä–µ—à–∫–∞",
                    "–û—Å–æ–±–µ–Ω–æ—Å—Ç–∏": "HTML –≥—Ä–µ—à–∫–∞", "–ü—Ä–æ–¥–∞–≤–∞—á": "HTML –≥—Ä–µ—à–∫–∞",
                    "–¢–µ–ª–µ—Ñ–æ–Ω –Ω–∞ –ø—Ä–æ–¥–∞–≤–∞—á–∞": "HTML –≥—Ä–µ—à–∫–∞", "–û–ø–∏—Å–∞–Ω–∏–µ": "HTML –≥—Ä–µ—à–∫–∞",
                    "–°—ä—Å—Ç–æ—è–Ω–∏–µ": "HTML –≥—Ä–µ—à–∫–∞",
                    "–°–∫—Ä–∏–π–Ω—à–æ—Ç": "‚ùå –ù—è–º–∞ HTML –∑–∞ —Å–∫—Ä–∏–π–Ω—à–æ—Ç" if SCREENSHOTS_FOLDER_ID else "‚ö†Ô∏è –ù—è–º–∞ –ø–∞–ø–∫–∞ –∑–∞ —Å–∫—Ä–∏–π–Ω—à–æ—Ç–∏",
                    "–¶–µ–Ω–∞ –Ω–∞ —Å–¥–µ–ª–∫–∞—Ç–∞": "", "–ö–æ–º–µ–Ω—Ç–∞—Ä–∏": "", "URL –∑–∞ –∞–Ω–∞–ª–∏–∑": ad_url
                }
                accumulated_data.append(error_data)
                existing_urls.add(ad_url)
            else:
                analysis_result = analyze_property_complete(ad_url, html_content)
                property_data = analysis_result["data"]

                if not analysis_result.get("success"):
                    print(f"‚ùå –ß–∞—Å—Ç–∏—á–Ω–∞ –≥—Ä–µ—à–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑ –Ω–∞ –æ–±—è–≤–∞—Ç–∞")

                print(f"\nüì∏ –ü–†–ê–í–Ø –°–ö–†–ò–ô–ù–®–û–¢ –ò –ö–ê–ß–í–ê–ú –í –ü–ê–ü–ö–ê ScreenShotsScraper: {ad_url}")

                if SCREENSHOTS_FOLDER_ID:
                    screenshot_link = await screenshot_final(ad_url)

                    if screenshot_link:
                        property_data["–°–∫—Ä–∏–π–Ω—à–æ—Ç"] = f'=HYPERLINK("{screenshot_link}","üì∏ –í–∏–∂ —Å–∫—Ä–∏–π–Ω—à–æ—Ç")'
                        print(f"‚úÖ –°–∫—Ä–∏–π–Ω—à–æ—Ç hyperlink –≥–æ—Ç–æ–≤: {screenshot_link}")
                    else:
                        property_data["–°–∫—Ä–∏–π–Ω—à–æ—Ç"] = "‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ —Å–∫—Ä–∏–π–Ω—à–æ—Ç"
                        print(f"‚ùå –ù–µ —É—Å–ø—è—Ö –¥–∞ –Ω–∞–ø—Ä–∞–≤—è —Å–∫—Ä–∏–π–Ω—à–æ—Ç")
                else:
                    print("‚ö†Ô∏è –ù—è–º–∞ –ø–∞–ø–∫–∞ –∑–∞ —Å–∫—Ä–∏–π–Ω—à–æ—Ç–∏ - –ø—Ä–µ—Å–∫–∞—á–∞–º —Å–∫—Ä–∏–π–Ω—à–æ—Ç–∞")
                    property_data["–°–∫—Ä–∏–π–Ω—à–æ—Ç"] = "‚ö†Ô∏è –ù—è–º–∞ –ø–∞–ø–∫–∞ –∑–∞ —Å–∫—Ä–∏–π–Ω—à–æ—Ç–∏"

                accumulated_data.append(property_data)
                existing_urls.add(ad_url)

            catalog_processed_ads += 1

            if len(accumulated_data) >= BATCH_SIZE:
                batch_counter += 1
                print(f"\n" + "üöÄ"*60)
                print(f"üìä –ó–ê–ü–ò–°–í–ê–ú BATCH #{batch_counter} - {len(accumulated_data)} –û–ë–Ø–í–ò –í GOOGLE SHEETS")
                print("üöÄ"*60)

                success = save_data_to_sheets(google_sheet_name, accumulated_data)

                if success:
                    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞—Ö batch #{batch_counter} —Å {len(accumulated_data)} –æ–±—è–≤–∏")
                    accumulated_data = []
                else:
                    print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å –Ω–∞ batch #{batch_counter}")

            if index < len(new_ad_links) - 1:
                delay_seconds = random.uniform(0, 1)
                print(f"\n‚è≥ –ò–∑—á–∞–∫–≤–∞–º {delay_seconds:.1f} —Å–µ–∫. –ø—Ä–µ–¥–∏ —Å–ª–µ–¥–≤–∞—â–∞—Ç–∞ –æ–±—è–≤–∞...")
                await asyncio.sleep(delay_seconds)

        print(f"\nüîç –¢–™–†–°–Ø –°–õ–ï–î–í–ê–©–ê –°–¢–†–ê–ù–ò–¶–ê –°–õ–ï–î {page_number} –í –ö–ê–¢–ê–õ–û–ì '{catalog_name}'...")
        next_catalog_url = find_next_page_url(current_catalog_url)

        if next_catalog_url:
            current_catalog_url = next_catalog_url
            page_number += 1
            page_delay = random.uniform(3, 8)
            print(f"‚è≥ –ò–∑—á–∞–∫–≤–∞–º {page_delay:.1f} —Å–µ–∫. –ø—Ä–µ–¥–∏ —Å–ª–µ–¥–≤–∞—â–∞—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞...")
            await asyncio.sleep(page_delay)
        else:
            print(f"‚úÖ –ù—è–º–∞ –ø–æ–≤–µ—á–µ —Å—Ç—Ä–∞–Ω–∏—Ü–∏ –≤ –∫–∞—Ç–∞–ª–æ–≥ '{catalog_name}' - –ø—Ä–∏–∫–ª—é—á–∏—Ö —Å —Ç–æ–∑–∏ –∫–∞—Ç–∞–ª–æ–≥!")
            current_catalog_url = None

    if accumulated_data:
        batch_counter += 1
        print(f"\n" + "üèÅ"*60)
        print(f"üìä –ó–ê–ü–ò–°–í–ê–ú –§–ò–ù–ê–õ–ï–ù BATCH #{batch_counter} - {len(accumulated_data)} –û–°–¢–ê–ù–ê–õ–ò –û–ë–Ø–í–ò –ó–ê '{catalog_name}'")
        print("üèÅ"*60)

        success = save_data_to_sheets(google_sheet_name, accumulated_data)

        if success:
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞—Ö —Ñ–∏–Ω–∞–ª–µ–Ω batch #{batch_counter} —Å {len(accumulated_data)} –æ–±—è–≤–∏ –∑–∞ '{catalog_name}'")
        else:
            print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å –Ω–∞ —Ñ–∏–Ω–∞–ª–µ–Ω batch #{batch_counter} –∑–∞ '{catalog_name}'")

    print(f"\n" + "="*80)
    print(f"üèÅ –ó–ê–í–™–†–®–ò–• –ö–ê–¢–ê–õ–û–ì {catalog_index}/{total_catalogs}: {catalog_name}")
    print(f"üìä –û–±—Ä–∞–±–æ—Ç–µ–Ω–∏ –Ω–æ–≤–∏ –æ–±—è–≤–∏ –æ—Ç —Ç–æ–∑–∏ –∫–∞—Ç–∞–ª–æ–≥: {catalog_processed_ads}")
    print(f"‚è≠Ô∏è –ü—Ä–µ—Å–∫–æ—á–µ–Ω–∏ –æ–±—è–≤–∏ –æ—Ç —Ç–æ–∑–∏ –∫–∞—Ç–∞–ª–æ–≥: {catalog_skipped_ads}")
    print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–µ–Ω–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∏ –æ—Ç —Ç–æ–∑–∏ –∫–∞—Ç–∞–ª–æ–≥: {page_number}")
    print(f"üì¶ –û–±—â–æ batch –∑–∞–ø–∏—Å–≤–∞–Ω–∏—è –∑–∞ —Ç–æ–∑–∏ –∫–∞—Ç–∞–ª–æ–≥: {batch_counter}")
    print("="*80)

    return catalog_processed_ads, catalog_skipped_ads, page_number

async def main_catalog_ultimate():
    """–ì–õ–ê–í–ù–ê –§–£–ù–ö–¶–ò–Ø –ó–ê –ò–ó–í–õ–ò–ß–ê–ù–ï –û–¢ –ú–ù–û–ñ–ï–°–¢–í–û –ö–ê–¢–ê–õ–û–ó–ò –° –ü–†–û–í–ï–†–ö–ê –ó–ê –î–£–ë–õ–ò–†–ê–ù–ò URL-–ò"""
    print("üöÄ –°–¢–ê–†–¢–ò–†–ê–ù–ï –ù–ê –ú–£–õ–¢–ò–ö–ê–¢–ê–õ–û–ñ–ï–ù –ö–û–î - –ê–í–¢–û–ú–ê–¢–ò–ß–ù–û –ò–ó–í–õ–ò–ß–ê–ù–ï!")
    print("="*80)

    # –ó–∞–ø–∏—Å–≤–∞–º–µ service account JSON –æ—Ç –ø—Ä–æ–º–µ–Ω–ª–∏–≤–∞—Ç–∞ –Ω–∞ —Å—Ä–µ–¥–∞—Ç–∞ –≤—ä–≤ –≤—Ä–µ–º–µ–Ω–µ–Ω —Ñ–∞–π–ª
    if GOOGLE_SERVICE_ACCOUNT_JSON:
        try:
            with open(SERVICE_ACCOUNT_FILE_PATH, 'w') as f:
                json.dump(json.loads(GOOGLE_SERVICE_ACCOUNT_JSON), f)
            print(f"‚úÖ Service account JSON –∑–∞–ø–∏—Å–∞–Ω –≤—ä–≤ –≤—Ä–µ–º–µ–Ω–µ–Ω —Ñ–∞–π–ª: {SERVICE_ACCOUNT_FILE_PATH}")
            os.chmod(SERVICE_ACCOUNT_FILE_PATH, 0o600) # –ó–∞–¥–∞–≤–∞–Ω–µ –Ω–∞ –ø—Ä–∞–≤–∏–ª–Ω–∏ –ø–µ—Ä–º–∏—à—ä–Ω–∏
        except Exception as e:
            print(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–≤–∞–Ω–µ –Ω–∞ service account JSON: {e}")
            print("‚ùó –°–∫—Ä–∏–ø—Ç—ä—Ç –º–æ–∂–µ –¥–∞ –Ω–µ –º–æ–∂–µ –¥–∞ –¥–æ—Å—Ç—ä–ø–∏ Google Sheets/Drive –±–µ–∑ —Ç–æ–≤–∞.")
            return # –ü—Ä–µ–∫—ä—Å–≤–∞–º–µ –∏–∑–ø—ä–ª–Ω–µ–Ω–∏–µ—Ç–æ, –∞–∫–æ –Ω–µ –º–æ–∂–µ–º –¥–∞ –Ω–∞—Å—Ç—Ä–æ–∏–º Service Account

    # –ü–™–†–í–û - –ø—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –¥–æ—Å—Ç—ä–ø–∞ –¥–æ —Ç–∞–±–ª–∏—Ü–∞—Ç–∞
    print("\nüõ†Ô∏è –ü–†–û–í–ï–†–ö–ê –ù–ê –î–û–°–¢–™–ü–ê –î–û –¢–ê–ë–õ–ò–¶–ê–¢–ê...")
    if not check_sheet_access(GOOGLE_SHEET_NAME):
        print("‚ùå –ì–†–ï–®–ö–ê –ü–†–ò –î–û–°–¢–™–ü –î–û –¢–ê–ë–õ–ò–¶–ê–¢–ê!")
        return

    # –ù–û–í–û: –ù–∞–º–∏—Ä–∞–º–µ –ø–∞–ø–∫–∞—Ç–∞ ScreenShotsScraper –≤ Google Drive
    print("\nüìÅ –ù–ê–°–¢–†–û–ô–ö–ê –ù–ê –ü–ê–ü–ö–ê –ó–ê –°–ö–†–ò–ô–ù–®–û–¢–ò...")
    screenshots_folder_id = find_screenshots_folder_id()
    if not screenshots_folder_id:
        print("‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ù–µ –º–æ–∂–∞—Ö –¥–∞ –Ω–∞—Å—Ç—Ä–æ—è –ø–∞–ø–∫–∞ –∑–∞ —Å–∫—Ä–∏–π–Ω—à–æ—Ç–∏!")
        print("‚ö†Ô∏è –°–∫—Ä–∏–π–Ω—à–æ—Ç–∏—Ç–µ –Ω—è–º–∞ –¥–∞ —Å–µ –ø—Ä–∞–≤—è—Ç, –Ω–æ –¥–∞–Ω–Ω–∏—Ç–µ —â–µ —Å–µ –∏–∑–≤–ª–∏—á–∞—Ç!")
    else:
        print(f"‚úÖ –ü–∞–ø–∫–∞—Ç–∞ –∑–∞ —Å–∫—Ä–∏–π–Ω—à–æ—Ç–∏ –µ –≥–æ—Ç–æ–≤–∞ —Å ID: {screenshots_folder_id}")

    print("\nüîç –ò–ó–í–õ–ò–ß–ê–ú –°–™–©–ï–°–¢–í–£–í–ê–©–ò URL-–ò –ó–ê –ü–†–û–í–ï–†–ö–ê –ù–ê –î–£–ë–õ–ò–†–ê–ù–ò...")
    existing_urls = get_existing_urls_from_sheet(GOOGLE_SHEET_NAME)
    print(f"üìã –ó–∞—Ä–µ–¥–∏—Ö {len(existing_urls)} —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—â–∏ URL-–∏ –∑–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞")

    print(f"\nüìã –ü–õ–ê–ù –ó–ê –û–ë–†–ê–ë–û–¢–ö–ê –ù–ê {len(CATALOG_URLS)} –ö–ê–¢–ê–õ–û–ì–ê:")
    for i, catalog_url in enumerate(CATALOG_URLS, 1):
        catalog_name = get_catalog_name_from_url(catalog_url)
        print(f" ¬† {i}. {catalog_name}")

    total_processed_ads = 0
    total_skipped_ads = 0
    total_pages = 0

    for catalog_index, catalog_url in enumerate(CATALOG_URLS, 1):
        try:
            catalog_stats = await process_single_catalog(
                catalog_url,
                catalog_index,
                len(CATALOG_URLS),
                existing_urls,
                GOOGLE_SHEET_NAME
            )

            catalog_processed, catalog_skipped, catalog_pages = catalog_stats
            total_processed_ads += catalog_processed
            total_skipped_ads += catalog_skipped
            total_pages += catalog_pages

            if catalog_index < len(CATALOG_URLS):
                catalog_name = get_catalog_name_from_url(catalog_url)
                next_catalog_name = get_catalog_name_from_url(CATALOG_URLS[catalog_index])

                catalog_delay = random.uniform(5, 15)
                print(f"\n‚è≥ –ò–ó–ß–ê–ö–í–ê–ú {catalog_delay:.1f} –°–ï–ö. –ü–†–ï–î–ò –°–õ–ï–î–í–ê–©–ò–Ø –ö–ê–¢–ê–õ–û–ì...")
                print(f"üîÑ –°–õ–ï–î–í–ê–© –ö–ê–¢–ê–õ–û–ì: {next_catalog_name}")
                await asyncio.sleep(catalog_delay)

        except Exception as e:
            catalog_name = get_catalog_name_from_url(catalog_url)
            print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ù–ê –ì–†–ï–®–ö–ê –í –ö–ê–¢–ê–õ–û–ì '{catalog_name}': {e}")
            print(traceback.format_exc())
            print(f"‚è≠Ô∏è –ü–†–ï–ú–ò–ù–ê–í–ê–ú –ö–™–ú –°–õ–ï–î–í–ê–©–ò–Ø –ö–ê–¢–ê–õ–û–ì...")
            continue

    print(f"\n" + "="*80)
    print(f"üèÅüéâ –ó–ê–í–™–†–®–ò–• –ò–ó–í–õ–ò–ß–ê–ù–ï–¢–û –û–¢ –í–°–ò–ß–ö–ò {len(CATALOG_URLS)} –ö–ê–¢–ê–õ–û–ì–ê! üéâüèÅ")
    print("="*80)
    print(f"üìä –û–ë–©–ò –°–¢–ê–¢–ò–°–¢–ò–ö–ò:")
    print(f" ¬† ‚úÖ –û–±—â–æ –ù–û–í–ò –æ–±—Ä–∞–±–æ—Ç–µ–Ω–∏ –æ–±—è–≤–∏: {total_processed_ads}")
    print(f" ¬† ‚è≠Ô∏è –û–±—â–æ –ü–†–ï–°–ö–û–ß–ï–ù–ò –æ–±—è–≤–∏ (–¥—É–±–ª–∏—Ä–∞–Ω–∏): {total_skipped_ads}")
    print(f" ¬† üìÑ –û–±—â–æ –æ–±—Ä–∞–±–æ—Ç–µ–Ω–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∏: {total_pages}")
    print(f" ¬† üè™ –û–±—â–æ –æ–±—Ä–∞–±–æ—Ç–µ–Ω–∏ –∫–∞—Ç–∞–ª–æ–∑–∏: {len(CATALOG_URLS)}")
    print(f" ¬† üì¶ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: Batch —Ä–∞–∑–º–µ—Ä {BATCH_SIZE} –æ–±—è–≤–∏ = ~{13}x –ø–æ-–±—ä—Ä–∑ –∑–∞–ø–∏—Å!")
    print(f"üìã –ü—Ä–æ–≤–µ—Ä–µ—Ç–µ —Ç–∞–±–ª–∏—Ü–∞—Ç–∞ '{GOOGLE_SHEET_NAME}' –≤ Google Sheets")
    print(f"‚úÖ –í—Å–∏—á–∫–∏ –Ω–æ–≤–∏ –¥–∞–Ω–Ω–∏ —Å–∞ –¥–æ–±–∞–≤–µ–Ω–∏ –∫—ä–º —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—â–∏—Ç–µ –∑–∞–ø–∏—Å–∏")
    print(f"‚úÖ –ö–æ–ª–æ–Ω–∞ B: –±–µ–∑ '–≥—Ä–∞–¥ –í–∞—Ä–Ω–∞, '")
    print(f"‚úÖ –ö–æ–ª–æ–Ω–∞ E: —Ñ–æ—Ä–º—É–ª–∞ =ROUND(C/D,0)")
    print(f"‚úÖ –ö–æ–ª–æ–Ω–∞ F: –±–µ–∑ '–ï—Ç–∞–∂:'")
    print(f"‚úÖ –ö–æ–ª–æ–Ω–∞ O: {'–∫–ª–∏–∫–∞–µ–º–∏ –ª–∏–Ω–∫–æ–≤–µ –∫—ä–º –°–ö–†–ò–ô–ù–®–û–¢–ò –≤ –ø–∞–ø–∫–∞ ScreenShotsScraper' if SCREENSHOTS_FOLDER_ID else '–ù–Ø–ú–ê –°–ö–†–ò–ô–ù–®–û–¢–ò (–ø—Ä–æ–±–ª–µ–º —Å –ø–∞–ø–∫–∞—Ç–∞)'}")
    print(f"‚úÖ –ö–æ–ª–æ–Ω–∞ R: URL –∞–¥—Ä–µ—Å–∏")
    if SCREENSHOTS_FOLDER_ID:
        print(f"üì∏ –°–∫—Ä–∏–π–Ω—à–æ—Ç–∏—Ç–µ —Å–∞ –∫–∞—á–µ–Ω–∏ –≤ –ø–∞–ø–∫–∞ 'ScreenShotsScraper' –≤ Google Drive")
    else:
        print(f"‚ö†Ô∏è –°–ö–†–ò–ô–ù–®–û–¢–ò–¢–ï –ù–ï –°–ê –ù–ê–ü–†–ê–í–ï–ù–ò –ø–æ—Ä–∞–¥–∏ –ø—Ä–æ–±–ª–µ–º —Å –ø–∞–ø–∫–∞—Ç–∞ –≤ Google Drive")
    print(f"üîÑ –ê–í–¢–û–ú–ê–¢–ò–ß–ù–ê –ü–†–û–í–ï–†–ö–ê –ó–ê –î–£–ë–õ–ò–†–ê–ù–ò URL-–ò –†–ê–ë–û–¢–ò –ü–ï–†–§–ï–ö–¢–ù–û!")
    print(f"üöÄ BATCH –õ–û–ì–ò–ö–ê: {BATCH_SIZE} –æ–±—è–≤–∏ –Ω–∞–≤–µ–¥–Ω—ä–∂ = –ó–ù–ê–ß–ò–¢–ï–õ–ù–û –ü–û-–ë–™–†–ó –ó–ê–ü–ò–°!")

    print(f"\nüè™ –û–ë–†–ê–ë–û–¢–ï–ù–ò –ö–ê–¢–ê–õ–û–ó–ò:")
    for i, catalog_url in enumerate(CATALOG_URLS, 1):
        catalog_name = get_catalog_name_from_url(catalog_url)
        print(f" ¬† ‚úÖ {i}. {catalog_name}")

    print("="*80)

def run_scraper_scheduled():
    """–§—É–Ω–∫—Ü–∏—è, –∫–æ—è—Ç–æ —â–µ —Å–µ –∏–∑–ø—ä–ª–Ω—è–≤–∞ –æ—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤—á–∏–∫–∞"""
    print(f"\n--- –°–¢–ê–†–¢–ò–†–ê–ù–ï –ù–ê –ü–õ–ê–ù–ò–†–ê–ù–ê –ó–ê–î–ê–ß–ê –ó–ê –°–ö–†–ï–ô–ü–í–ê–ù–ï (–Ω–∞ –≤—Å–µ–∫–∏ {SCHEDULE_HOURS} —á–∞—Å–∞) ---")
    asyncio.run(main_catalog_ultimate())
    print(f"--- –ü–õ–ê–ù–ò–†–ê–ù–ê –ó–ê–î–ê–ß–ê –ó–ê –°–ö–†–ï–ô–ü–í–ê–ù–ï –ü–†–ò–ö–õ–Æ–ß–ò ---")

if __name__ == "__main__":
    print("üÜï –°–¢–ê–†–¢–ò–†–ê–ú –ì–õ–ê–í–ù–û –ü–†–ò–õ–û–ñ–ï–ù–ò–ï –ó–ê ELTEST.IO / DOCKER!")
    print("üìÅ –°–ö–†–ò–ô–ù–®–û–¢–ò–¢–ï –©–ï –°–ï –ó–ê–ü–ò–°–í–ê–¢ –í –ü–ê–ü–ö–ê ScreenShotsScraper –í GOOGLE DRIVE!")

    # –°—Ç–∞—Ä—Ç–∏—Ä–∞–º–µ Flask —Å—ä—Ä–≤—ä—Ä–∞ –≤ –æ—Ç–¥–µ–ª–Ω–∞ –Ω–∏—à–∫–∞
    print(f"üåê –°—Ç–∞—Ä—Ç–∏—Ä–∞–º Flask healthcheck —Å—ä—Ä–≤—ä—Ä –Ω–∞ –ø–æ—Ä—Ç {WEB_SERVER_PORT}...")
    threading.Thread(target=lambda: app.run(host="0.0.0.0", port=WEB_SERVER_PORT, debug=False, use_reloader=False)).start()

    # –ò–∑–ø—ä–ª–Ω—è–≤–∞–º–µ —Å–∫—Ä–∏–ø—Ç–∞ –≤–µ–¥–Ω–∞–≥–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ
    print("\n--- –ò–ó–ü–™–õ–ù–Ø–í–ê–ú –°–ö–†–ï–ô–ü–™–†–ê –í–ï–î–ù–ê–ì–ê –ü–†–ò –°–¢–ê–†–¢ ---")
    asyncio.run(main_catalog_ultimate())

    # –°–ª–µ–¥ —Ç–æ–≤–∞ –ø–ª–∞–Ω–∏—Ä–∞–º–µ –∏–∑–ø—ä–ª–Ω–µ–Ω–∏–µ—Ç–æ
    print(f"\n--- –ù–ê–°–¢–†–û–ô–í–ê–ú –ü–õ–ê–ù–ò–†–ê–ù–ï –ù–ê –°–ö–†–ï–ô–ü–™–†–ê –ù–ê –í–°–ï–ö–ò {SCHEDULE_HOURS} –ß–ê–°–ê ---")
    schedule.every(SCHEDULE_HOURS).hours.do(run_scraper_scheduled)

    # –ë–µ–∑–∫—Ä–∞–µ–Ω —Ü–∏–∫—ä–ª –∑–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤—á–∏–∫–∞
    while True:
        schedule.run_pending()
        time.sleep(1) # –ü—Ä–æ–≤–µ—Ä—è–≤–∞ –Ω–∞ –≤—Å—è–∫–∞ —Å–µ–∫—É–Ω–¥–∞
```
